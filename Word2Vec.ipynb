{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. `gensim` Original Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# logging.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals import Evaluation\n",
    "from datetime import datetime\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec, LineSentence\n",
    "\n",
    "Little_Wiki_Path = 'corpus/wiki/wiki.txt'\n",
    "Total_Wiki_Path = 'corpus/WikiTotal/WikiTotal7k_v2.txt'\n",
    "\n",
    "data_input = LineSentence(Little_Wiki_Path)\n",
    "data_input = LineSentence(Total_Wiki_Path)\n",
    "\n",
    "\n",
    "D = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-06-10 21:36:27.655403\n",
      "+++++End++++++ 2019-06-10 21:46:45.450065 Using: 0:10:17.794662\n",
      "0:00:18.246085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4973762729287117,\n",
       " 'sim297_spearman': 0.5817167595842151,\n",
       " 'ana_capital-common-countries': 0.6255411255411255,\n",
       " 'ana_city-in-state': 0.6,\n",
       " 'ana_family': 0.5,\n",
       " 'ana_Total accuracy': 0.583058305830583,\n",
       " 'time': datetime.timedelta(seconds=617, microseconds=794662)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, \n",
    "                    sg = 1, iter=1, workers = 8)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "\n",
    "e = datetime.now()\n",
    "\n",
    "print(e - s)\n",
    "D['gensim_sg'] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, \n",
    "                    sg = 0, iter=1, workers = 8)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "D['gensim_cbow'] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. `fieldembed.Word2Vec` Original Gensim Word2Vec in FieldEmbed\n",
    "\n",
    "## 1.1 SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "\n",
    "Little_Wiki_Path = 'corpus/wiki/wiki.txt'\n",
    "Total_Wiki_Path = 'corpus/WikiTotal/WikiTotal7k_v2.txt'\n",
    "\n",
    "data_input = LineSentence(Little_Wiki_Path)\n",
    "data_input = LineSentence(Total_Wiki_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-17 00:06:16.015624\n",
      "======== Build_vocab based on LineSentence....\n",
      "o--> Get Vocab Frequency from Scanning Vocab\n",
      "\tStart:  2019-05-17 00:06:16.016234\n",
      "\tEnd  :  2019-05-17 00:07:25.085684\n",
      "\tTotal Time: 0:01:09.069450\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-17 00:07:25.086100\n",
      "\tEnd  :  2019-05-17 00:07:27.083207\n",
      "\tTotal Time: 0:00:01.997107\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-17 00:07:27.083326\n",
      "\tEnd  :  2019-05-17 00:07:28.089583\n",
      "\tTotal Time: 0:00:01.006257\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-17 00:07:28.614292\n",
      "\tEnd  :  2019-05-17 00:07:29.416776\n",
      "\tTotal Time: 0:00:00.802484\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-17 00:07:29.422573\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-17 00:07:33.663871\n",
      "\tTotal Time: 0:00:04.241298\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:01:17.647852\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "======== Training End ......\n",
      "======== Total Time:  0:08:36.695993\n",
      "+++++End++++++ 2019-05-17 00:16:10.475739 Using: 0:09:54.460115\n",
      "0:00:17.952860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.49974965180260594,\n",
       " 'sim297_spearman': 0.5703043271841864,\n",
       " 'ana_capital-common-countries': 0.6103896103896104,\n",
       " 'ana_city-in-state': 0.5828571428571429,\n",
       " 'ana_family': 0.4742647058823529,\n",
       " 'ana_Total accuracy': 0.5643564356435643,\n",
       " 'time': datetime.timedelta(seconds=594, microseconds=460115)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, \n",
    "                 sg = 1, iter=1, workers = 8)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D['gensim_my_sg'] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-16 23:59:39.106055\n",
      "======== Build_vocab based on LineSentence....\n",
      "o--> Get Vocab Frequency from Scanning Vocab\n",
      "\tStart:  2019-05-16 23:59:39.106966\n",
      "\tEnd  :  2019-05-17 00:00:46.352658\n",
      "\tTotal Time: 0:01:07.245692\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-17 00:00:46.352794\n",
      "\tEnd  :  2019-05-17 00:00:48.622159\n",
      "\tTotal Time: 0:00:02.269365\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-17 00:00:48.622321\n",
      "\tEnd  :  2019-05-17 00:00:49.701027\n",
      "\tTotal Time: 0:00:01.078706\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-17 00:00:50.239192\n",
      "\tEnd  :  2019-05-17 00:00:51.053769\n",
      "\tTotal Time: 0:00:00.814577\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-17 00:00:51.059452\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-17 00:00:55.395421\n",
      "\tTotal Time: 0:00:04.335969\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:01:16.288643\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "======== Training End ......\n",
      "======== Total Time:  0:03:50.115800\n",
      "+++++End++++++ 2019-05-17 00:04:45.511538 Using: 0:05:06.405483\n",
      "0:00:17.912777\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4712488546428894,\n",
       " 'sim297_spearman': 0.5842397755421264,\n",
       " 'ana_capital-common-countries': 0.5021645021645021,\n",
       " 'ana_city-in-state': 0.38857142857142857,\n",
       " 'ana_family': 0.43014705882352944,\n",
       " 'ana_Total accuracy': 0.45874587458745875,\n",
       " 'time': datetime.timedelta(seconds=306, microseconds=405483)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, \n",
    "                 sg = 0, iter=1, workers = 8, cbow_mean=1)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "e = datetime.now()\n",
    "d['time'] = time\n",
    "\n",
    "print(e - s)\n",
    "D['gensim_my_cbow'] = d\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. `fieldembed.word2vec` Field Embedding With NLPText\n",
    "\n",
    "## 2.1 SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447174/GrainUnique/token.p\n",
      "token\tthe length of it is   : 447174\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-17 00:34:23.968828\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-17 00:34:23.976346\n",
      "\tEnd  :  2019-05-17 00:34:24.729903\n",
      "\tTotal Time: 0:00:00.753557\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-17 00:34:24.730026\n",
      "\tEnd  :  2019-05-17 00:34:25.786698\n",
      "\tTotal Time: 0:00:01.056672\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-17 00:34:25.787107\n",
      "\tEnd  :  2019-05-17 00:34:26.445804\n",
      "\tTotal Time: 0:00:00.658697\n",
      "-------> Prepare Field Info....\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-17 00:34:26.446017\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-17 00:34:30.732342\n",
      "\tTotal Time: 0:00:04.286325\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.762830\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-17 00:34:30.732560\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-17 00:34:30.749272\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.016712\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:07:46.617853\n",
      "+++++End++++++ 2019-05-17 00:42:17.371147 Using: 0:07:53.402319\n",
      "0:00:17.212016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.49862371438461517,\n",
       " 'sim297_spearman': 0.5894198712949252,\n",
       " 'ana_capital-common-countries': 0.5735930735930735,\n",
       " 'ana_city-in-state': 0.5485714285714286,\n",
       " 'ana_family': 0.4852941176470588,\n",
       " 'ana_Total accuracy': 0.5423542354235423,\n",
       " 'time': datetime.timedelta(seconds=473, microseconds=402319)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_words = 10000\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(nlptext = BasicObject, size = 200, alpha = 0.025, sg = 1, iter=1, workers = 8)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "D['nlptext_sg'] = d\n",
    "\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:18.210325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4774263224370997,\n",
       " 'sim297_spearman': 0.582609003320115,\n",
       " 'ana_capital-common-countries': 0.5064935064935064,\n",
       " 'ana_city-in-state': 0.37714285714285717,\n",
       " 'ana_family': 0.4485294117647059,\n",
       " 'ana_Total accuracy': 0.46424642464246424,\n",
       " 'time': datetime.timedelta(seconds=147, microseconds=244866)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_words = 10000\n",
    "\n",
    "\n",
    "standard_grad = 0 # means large stepsize\n",
    "\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(nlptext = BasicObject, size = 200, alpha = 0.025, sg = 0, iter=1, workers = 8, standard_grad = standard_grad)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "\n",
    "\n",
    "from evals import Evaluation\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "D['nlptext_cbow'] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.44351505172249667,\n",
       " 'sim297_spearman': 0.511224926368528,\n",
       " 'ana_capital-common-countries': 0.21861471861471862,\n",
       " 'ana_city-in-state': 0.2,\n",
       " 'ana_family': 0.40808823529411764,\n",
       " 'ana_Total accuracy': 0.2717271727172717,\n",
       " 'time': datetime.timedelta(seconds=149, microseconds=984233)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_words = 10000\n",
    "\n",
    "\n",
    "standard_grad = 1 # means large stepsize\n",
    "\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = Word2Vec(nlptext = BasicObject, size = 200, alpha = 0.025, sg = 0, iter=1, workers = 8, \n",
    "                 standard_grad = standard_grad)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "D['nlptext_cbow_std_grad'] = d\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nlptext_cbow</th>\n",
       "      <th>nlptext_cbow_std_grad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ana_Total accuracy</th>\n",
       "      <td>0.464246</td>\n",
       "      <td>0.271727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ana_capital-common-countries</th>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.218615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ana_city-in-state</th>\n",
       "      <td>0.377143</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ana_family</th>\n",
       "      <td>0.448529</td>\n",
       "      <td>0.408088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim240_spearman</th>\n",
       "      <td>0.477426</td>\n",
       "      <td>0.443515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sim297_spearman</th>\n",
       "      <td>0.582609</td>\n",
       "      <td>0.511225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0:02:27.244866</td>\n",
       "      <td>0:02:29.984233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                nlptext_cbow nlptext_cbow_std_grad\n",
       "ana_Total accuracy                  0.464246              0.271727\n",
       "ana_capital-common-countries        0.506494              0.218615\n",
       "ana_city-in-state                   0.377143                   0.2\n",
       "ana_family                          0.448529              0.408088\n",
       "sim240_spearman                     0.477426              0.443515\n",
       "sim297_spearman                     0.582609              0.511225\n",
       "time                          0:02:27.244866        0:02:29.984233"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. `fieldembed.FieldEmbedding` About `fieldembed_nlptext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-17 01:14:38.809108\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BasicObject' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-506b5c48c687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+++++Start++++++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s );\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n\u001b[0m\u001b[1;32m     18\u001b[0m                        \u001b[0mstandard_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandard_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BasicObject' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "standard_grad = 1 # means large stepsize\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                       standard_grad = standard_grad,\n",
    "                        sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447174/GrainUnique/token.p\n",
      "token\tthe length of it is   : 447174\n",
      "**************************************** \n",
      "\n",
      "+++++Start++++++ 2019-05-17 01:36:34.667298\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-17 01:36:34.670509\n",
      "\tEnd  :  2019-05-17 01:36:35.521407\n",
      "\tTotal Time: 0:00:00.850898\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-17 01:36:35.521538\n",
      "\tEnd  :  2019-05-17 01:36:36.464887\n",
      "\tTotal Time: 0:00:00.943349\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-17 01:36:36.465263\n",
      "\tEnd  :  2019-05-17 01:36:37.246595\n",
      "\tTotal Time: 0:00:00.781332\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7f940e861940>]]\n",
      "[[]]\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-17 01:36:37.247028\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-17 01:36:41.191666\n",
      "\tTotal Time: 0:00:03.944638\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.524398\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-17 01:36:41.191977\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-17 01:36:41.208437\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.016460\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:03:14.874922\n",
      "+++++End++++++ 2019-05-17 01:39:56.067032 Using: 0:03:21.399734\n",
      "0:00:27.925541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.43797902399510036,\n",
       " 'sim297_spearman': 0.5194422559695888,\n",
       " 'ana_capital-common-countries': 0.22943722943722944,\n",
       " 'ana_city-in-state': 0.21142857142857144,\n",
       " 'ana_family': 0.3897058823529412,\n",
       " 'ana_Total accuracy': 0.2739273927392739,\n",
       " 'time': datetime.timedelta(seconds=201, microseconds=399734)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "D = {}\n",
    "\n",
    "from evals import Evaluation\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "standard_grad = 1\n",
    "\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                       standard_grad = standard_grad,\n",
    "                       sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447174/GrainUnique/token.p\n",
      "token\tthe length of it is   : 447174\n",
      "**************************************** \n",
      "\n",
      "+++++Start++++++ 2019-05-20 17:37:55.363073\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-20 17:37:55.367356\n",
      "\tEnd  :  2019-05-20 17:37:56.212013\n",
      "\tTotal Time: 0:00:00.844657\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-20 17:37:56.212184\n",
      "\tEnd  :  2019-05-20 17:37:57.156669\n",
      "\tTotal Time: 0:00:00.944485\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-20 17:37:57.156834\n",
      "\tEnd  :  2019-05-20 17:37:57.842327\n",
      "\tTotal Time: 0:00:00.685493\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7f9f20ba07f0>]]\n",
      "[[]]\n",
      "token (0, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-20 17:37:57.843248\n",
      "init neg with 0\n",
      "token (447174, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "\tEnd  :  2019-05-20 17:38:01.881575\n",
      "\tTotal Time: 0:00:04.038327\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.518294\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "257789077\n",
      "Start getting batch infos\n",
      "2019-05-20 17:38:01.882089\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-20 17:38:01.907936\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.025847\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:02:50.453327\n",
      "+++++End++++++ 2019-05-20 17:40:52.337203 Using: 0:02:56.974130\n",
      "0:00:25.026979\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-26290580f1b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msg_or_cbow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'D' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from evals import Evaluation\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "standard_grad = 0\n",
    "\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                       standard_grad = standard_grad,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4696541141452856,\n",
       " 'sim297_spearman': 0.5808741237776697,\n",
       " 'ana_capital-common-countries': 0.48484848484848486,\n",
       " 'ana_city-in-state': 0.3657142857142857,\n",
       " 'ana_family': 0.47058823529411764,\n",
       " 'ana_Total accuracy': 0.45764576457645767,\n",
       " 'time': datetime.timedelta(seconds=158, microseconds=230675)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. `0X1` Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-16 11:29:53.814744\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-16 11:29:53.821407\n",
      "\tEnd  :  2019-05-16 11:29:54.638119\n",
      "\tTotal Time: 0:00:00.816712\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-16 11:29:54.638243\n",
      "\tEnd  :  2019-05-16 11:29:55.647184\n",
      "\tTotal Time: 0:00:01.008941\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-16 11:29:55.647344\n",
      "\tEnd  :  2019-05-16 11:29:56.344505\n",
      "\tTotal Time: 0:00:00.697161\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7fa659fde320>]]\n",
      "[[]]\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-16 11:29:56.345394\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-16 11:30:00.457778\n",
      "\tTotal Time: 0:00:04.112384\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.642812\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-16 11:30:00.458141\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-16 11:30:00.464527\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.006386\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:10:23.201832\n",
      "+++++End++++++ 2019-05-16 11:40:23.681400 Using: 0:10:29.866656\n",
      "0:00:17.169462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.47875172898399704,\n",
       " 'sim297_spearman': 0.5777964160601089,\n",
       " 'ana_capital-common-countries': 0.5021645021645021,\n",
       " 'ana_city-in-state': 0.5885714285714285,\n",
       " 'ana_family': 0.4742647058823529,\n",
       " 'ana_Total accuracy': 0.5104510451045104,\n",
       " 'time': datetime.timedelta(seconds=629, microseconds=866656)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_0X1'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-17 00:56:46.014310\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'standard_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f262a7209a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n\u001b[1;32m     19\u001b[0m                        \u001b[0mstandard_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstandard_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                           sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+++++End++++++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Using:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'standard_grad'"
     ]
    }
   ],
   "source": [
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_0X1'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "standard_grad = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                       standard_grad=standard_grad,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_0X1'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "standard_grad = 0\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                       standard_grad=standard_grad,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow+'_stdg' + str(standard_gradard_gradard_grad)] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ana_Total accuracy</th>\n",
       "      <th>ana_capital-common-countries</th>\n",
       "      <th>ana_city-in-state</th>\n",
       "      <th>ana_family</th>\n",
       "      <th>sim240_spearman</th>\n",
       "      <th>sim297_spearman</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gensim_sg</th>\n",
       "      <td>0.567657</td>\n",
       "      <td>0.61039</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.481618</td>\n",
       "      <td>0.510271</td>\n",
       "      <td>0.573017</td>\n",
       "      <td>0:09:55.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_cbow</th>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.521645</td>\n",
       "      <td>0.365714</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0.47621</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0:05:18.418710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_my_sg</th>\n",
       "      <td>0.575358</td>\n",
       "      <td>0.616883</td>\n",
       "      <td>0.594286</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>0.515477</td>\n",
       "      <td>0.583469</td>\n",
       "      <td>0:10:04.542353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_my_cbow</th>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.521645</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.481536</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0:05:19.986750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlptext_sg</th>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.491422</td>\n",
       "      <td>0.577564</td>\n",
       "      <td>0:10:12.943225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlptext_cbow</th>\n",
       "      <td>0.39934</td>\n",
       "      <td>0.367965</td>\n",
       "      <td>0.331429</td>\n",
       "      <td>0.496324</td>\n",
       "      <td>0.448559</td>\n",
       "      <td>0.56466</td>\n",
       "      <td>0:02:31.671171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_token_sg</th>\n",
       "      <td>0.50165</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.480788</td>\n",
       "      <td>0.579785</td>\n",
       "      <td>0:10:18.048347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_token_cbow</th>\n",
       "      <td>0.39934</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.567541</td>\n",
       "      <td>0:02:29.682940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_sg</th>\n",
       "      <td>0.510451</td>\n",
       "      <td>0.502165</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.474265</td>\n",
       "      <td>0.478752</td>\n",
       "      <td>0.577796</td>\n",
       "      <td>0:10:29.866656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_cbow</th>\n",
       "      <td>0.232123</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.154286</td>\n",
       "      <td>0.400735</td>\n",
       "      <td>0.426136</td>\n",
       "      <td>0.500063</td>\n",
       "      <td>0:02:35.508361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ana_Total accuracy ana_capital-common-countries  \\\n",
       "gensim_sg                       0.567657                      0.61039   \n",
       "gensim_cbow                     0.473047                     0.521645   \n",
       "gensim_my_sg                    0.575358                     0.616883   \n",
       "gensim_my_cbow                  0.473047                     0.521645   \n",
       "nlptext_sg                      0.531353                     0.525974   \n",
       "nlptext_cbow                     0.39934                     0.367965   \n",
       "fieldembed_token_sg              0.50165                     0.484848   \n",
       "fieldembed_token_cbow            0.39934                     0.387446   \n",
       "fieldembed_0X1_sg               0.510451                     0.502165   \n",
       "fieldembed_0X1_cbow             0.232123                     0.162338   \n",
       "\n",
       "                      ana_city-in-state ana_family sim240_spearman  \\\n",
       "gensim_sg                      0.588571   0.481618        0.510271   \n",
       "gensim_cbow                    0.365714   0.459559         0.47621   \n",
       "gensim_my_sg                   0.594286   0.492647        0.515477   \n",
       "gensim_my_cbow                 0.371429   0.455882        0.481536   \n",
       "nlptext_sg                     0.617143   0.485294        0.491422   \n",
       "nlptext_cbow                   0.331429   0.496324        0.448559   \n",
       "fieldembed_token_sg            0.617143   0.455882        0.480788   \n",
       "fieldembed_token_cbow          0.342857   0.455882          0.4576   \n",
       "fieldembed_0X1_sg              0.588571   0.474265        0.478752   \n",
       "fieldembed_0X1_cbow            0.154286   0.400735        0.426136   \n",
       "\n",
       "                      sim297_spearman            time  \n",
       "gensim_sg                    0.573017  0:09:55.008024  \n",
       "gensim_cbow                  0.581395  0:05:18.418710  \n",
       "gensim_my_sg                 0.583469  0:10:04.542353  \n",
       "gensim_my_cbow               0.584906  0:05:19.986750  \n",
       "nlptext_sg                   0.577564  0:10:12.943225  \n",
       "nlptext_cbow                  0.56466  0:02:31.671171  \n",
       "fieldembed_token_sg          0.579785  0:10:18.048347  \n",
       "fieldembed_token_cbow        0.567541  0:02:29.682940  \n",
       "fieldembed_0X1_sg            0.577796  0:10:29.866656  \n",
       "fieldembed_0X1_cbow          0.500063  0:02:35.508361  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(D).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/wiki/word/Token6905/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/wiki/word/Token6905/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/wiki/word/Token6905/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 533\n",
      "SENT\tread from pickle file : data/wiki/word/Token6905/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 533\n",
      "TOKEN\tread from pickle file : data/wiki/word/Token6905/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 31271\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/wiki/word/Token6905/GrainUnique/token.p\n",
      "token\tthe length of it is   : 6905\n",
      "**************************************** \n",
      "\n",
      "data/wiki/word/Token6905\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "# BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "# LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus()\n",
    "print(corpus.TokenNum_Dir)\n",
    "\n",
    "#if os.path.isdir(corpus.Channel_Dir):\n",
    "#    shutil.rmtree(corpus.Channel_Dir)\n",
    "\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'pinyin':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "+++++Start++++++ 2019-05-20 00:50:55.087385\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-20 00:50:55.088823\n",
      "\tEnd  :  2019-05-20 00:50:55.103521\n",
      "\tTotal Time: 0:00:00.014698\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-20 00:50:55.103724\n",
      "\tEnd  :  2019-05-20 00:50:55.128386\n",
      "\tTotal Time: 0:00:00.024662\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-20 00:50:55.128605\n",
      "\tEnd  :  2019-05-20 00:50:55.142609\n",
      "\tTotal Time: 0:00:00.014004\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7fc51069d940>]]\n",
      "[[]]\n",
      "token (0, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-20 00:50:55.143356\n",
      "init neg with 0\n",
      "token (6905, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "\tEnd  :  2019-05-20 00:50:55.231968\n",
      "\tTotal Time: 0:00:00.088612\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:00.143896\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "31271\n",
      "Start getting batch infos\n",
      "2019-05-20 00:50:55.232141\n",
      "Read info from: data/wiki/word/Token6905/Pyramid/10000_Info.p\n",
      "2019-05-20 00:50:55.232523\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.000382\n",
      "Total job number is: 4\n",
      "======== Training End ......\n",
      "======== Total Time:  0:00:00.099464\n",
      "+++++End++++++ 2019-05-20 00:50:55.332293 Using: 0:00:00.244908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "selected_fields = ['token', 'char']\n",
    "mode =  'fieldembed_0X1'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "\n",
    "selected_fields_template = {k: v for k, v in CHANNEL_SETTINGS_TEMPLATE.items() if k in selected_fields}\n",
    "\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(selected_fields_template)\n",
    "# BasicObject.CHANNEL_SETTINGS\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "# wv = model.wv\n",
    "# evals = Evaluation(wv)\n",
    "\n",
    "# s = datetime.now()\n",
    "# d = evals.run_wv_lexical_evals()\n",
    "# d['time'] = time\n",
    "# e = datetime.now()\n",
    "# print(e - s)\n",
    "\n",
    "\n",
    "# field_string = '_'.join(selected_fields)\n",
    "# D[mode+'_' + sg_or_cbow + '-' + field_string] = d\n",
    "# d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fieldembed.models.fieldembed.FieldEmbedding at 0x7fc51069d860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/text.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00135529, -0.00178496,  0.00047153, ...,  0.00140859,\n",
       "         0.00051031,  0.00173024],\n",
       "       [-0.00039652, -0.00061021,  0.00124014, ...,  0.00215727,\n",
       "        -0.00154854, -0.00182892],\n",
       "       [-0.00170397, -0.0005509 , -0.00085764, ..., -0.00248977,\n",
       "        -0.00158943, -0.00184403],\n",
       "       ...,\n",
       "       [ 0.00932628, -0.02095524,  0.04472251, ...,  0.00346594,\n",
       "         0.01519658, -0.07042512],\n",
       "       [ 0.00081099, -0.00273483,  0.00679341, ..., -0.00028974,\n",
       "         0.00127404, -0.01163549],\n",
       "       [ 0.00266284, -0.00400148,  0.00915829, ...,  0.00015735,\n",
       "         0.00312438, -0.01022781]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = FieldEmbedding.load('models/test/char_stroke_pinyin.mdl')\n",
    "# model.wv_neg.n_similarity('')\n",
    "model.wv_char.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "+++++Start++++++ 2019-05-16 12:29:00.783681\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-16 12:29:00.788554\n",
      "\tEnd  :  2019-05-16 12:29:01.594333\n",
      "\tTotal Time: 0:00:00.805779\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-16 12:29:01.594793\n",
      "\tEnd  :  2019-05-16 12:29:02.577810\n",
      "\tTotal Time: 0:00:00.983017\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-16 12:29:02.578254\n",
      "\tEnd  :  2019-05-16 12:29:03.242410\n",
      "\tTotal Time: 0:00:00.664156\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7fa661cc70b8>]]\n",
      "[[]]\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-16 12:29:03.243222\n",
      "init neg with 0\n",
      "\tEnd  :  2019-05-16 12:29:07.174987\n",
      "\tTotal Time: 0:00:03.931765\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.391240\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-16 12:29:07.175250\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-16 12:29:07.180617\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.005367\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:02:27.954857\n",
      "+++++End++++++ 2019-05-16 12:31:35.145195 Using: 0:02:34.361514\n",
      "0:00:17.736992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.426429558924676,\n",
       " 'sim297_spearman': 0.5009554655249785,\n",
       " 'ana_capital-common-countries': 0.14935064935064934,\n",
       " 'ana_city-in-state': 0.14857142857142858,\n",
       " 'ana_family': 0.4007352941176471,\n",
       " 'ana_Total accuracy': 0.22442244224422442,\n",
       " 'time': datetime.timedelta(seconds=154, microseconds=361514)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_fields = ['token', 'char']\n",
    "mode =  'fieldembed_0X1'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "\n",
    "selected_fields_template = {k: v for k, v in CHANNEL_SETTINGS_TEMPLATE.items() if k in selected_fields}\n",
    "\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(selected_fields_template)\n",
    "# BasicObject.CHANNEL_SETTINGS\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "\n",
    "field_string = '_'.join(selected_fields)\n",
    "D[mode+'_' + sg_or_cbow + '-' + field_string] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ana_Total accuracy</th>\n",
       "      <th>ana_capital-common-countries</th>\n",
       "      <th>ana_city-in-state</th>\n",
       "      <th>ana_family</th>\n",
       "      <th>sim240_spearman</th>\n",
       "      <th>sim297_spearman</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gensim_sg</th>\n",
       "      <td>0.567657</td>\n",
       "      <td>0.61039</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.481618</td>\n",
       "      <td>0.510271</td>\n",
       "      <td>0.573017</td>\n",
       "      <td>0:09:55.008024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_cbow</th>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.521645</td>\n",
       "      <td>0.365714</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0.47621</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0:05:18.418710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_my_sg</th>\n",
       "      <td>0.575358</td>\n",
       "      <td>0.616883</td>\n",
       "      <td>0.594286</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>0.515477</td>\n",
       "      <td>0.583469</td>\n",
       "      <td>0:10:04.542353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim_my_cbow</th>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.521645</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.481536</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0:05:19.986750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlptext_sg</th>\n",
       "      <td>0.531353</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.491422</td>\n",
       "      <td>0.577564</td>\n",
       "      <td>0:10:12.943225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlptext_cbow</th>\n",
       "      <td>0.39934</td>\n",
       "      <td>0.367965</td>\n",
       "      <td>0.331429</td>\n",
       "      <td>0.496324</td>\n",
       "      <td>0.448559</td>\n",
       "      <td>0.56466</td>\n",
       "      <td>0:02:31.671171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_token_sg</th>\n",
       "      <td>0.50165</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.480788</td>\n",
       "      <td>0.579785</td>\n",
       "      <td>0:10:18.048347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_token_cbow</th>\n",
       "      <td>0.39934</td>\n",
       "      <td>0.387446</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.455882</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.567541</td>\n",
       "      <td>0:02:29.682940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_sg</th>\n",
       "      <td>0.510451</td>\n",
       "      <td>0.502165</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.474265</td>\n",
       "      <td>0.478752</td>\n",
       "      <td>0.577796</td>\n",
       "      <td>0:10:29.866656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_cbow</th>\n",
       "      <td>0.232123</td>\n",
       "      <td>0.162338</td>\n",
       "      <td>0.154286</td>\n",
       "      <td>0.400735</td>\n",
       "      <td>0.426136</td>\n",
       "      <td>0.500063</td>\n",
       "      <td>0:02:35.508361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_sg-token_char</th>\n",
       "      <td>0.517052</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.459559</td>\n",
       "      <td>0.483109</td>\n",
       "      <td>0.572797</td>\n",
       "      <td>0:10:33.487445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fieldembed_0X1_cbow-token_char</th>\n",
       "      <td>0.224422</td>\n",
       "      <td>0.149351</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>0.400735</td>\n",
       "      <td>0.42643</td>\n",
       "      <td>0.500955</td>\n",
       "      <td>0:02:34.361514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ana_Total accuracy  \\\n",
       "gensim_sg                                0.567657   \n",
       "gensim_cbow                              0.473047   \n",
       "gensim_my_sg                             0.575358   \n",
       "gensim_my_cbow                           0.473047   \n",
       "nlptext_sg                               0.531353   \n",
       "nlptext_cbow                              0.39934   \n",
       "fieldembed_token_sg                       0.50165   \n",
       "fieldembed_token_cbow                     0.39934   \n",
       "fieldembed_0X1_sg                        0.510451   \n",
       "fieldembed_0X1_cbow                      0.232123   \n",
       "fieldembed_0X1_sg-token_char             0.517052   \n",
       "fieldembed_0X1_cbow-token_char           0.224422   \n",
       "\n",
       "                               ana_capital-common-countries ana_city-in-state  \\\n",
       "gensim_sg                                           0.61039          0.588571   \n",
       "gensim_cbow                                        0.521645          0.365714   \n",
       "gensim_my_sg                                       0.616883          0.594286   \n",
       "gensim_my_cbow                                     0.521645          0.371429   \n",
       "nlptext_sg                                         0.525974          0.617143   \n",
       "nlptext_cbow                                       0.367965          0.331429   \n",
       "fieldembed_token_sg                                0.484848          0.617143   \n",
       "fieldembed_token_cbow                              0.387446          0.342857   \n",
       "fieldembed_0X1_sg                                  0.502165          0.588571   \n",
       "fieldembed_0X1_cbow                                0.162338          0.154286   \n",
       "fieldembed_0X1_sg-token_char                       0.515152          0.611429   \n",
       "fieldembed_0X1_cbow-token_char                     0.149351          0.148571   \n",
       "\n",
       "                               ana_family sim240_spearman sim297_spearman  \\\n",
       "gensim_sg                        0.481618        0.510271        0.573017   \n",
       "gensim_cbow                      0.459559         0.47621        0.581395   \n",
       "gensim_my_sg                     0.492647        0.515477        0.583469   \n",
       "gensim_my_cbow                   0.455882        0.481536        0.584906   \n",
       "nlptext_sg                       0.485294        0.491422        0.577564   \n",
       "nlptext_cbow                     0.496324        0.448559         0.56466   \n",
       "fieldembed_token_sg              0.455882        0.480788        0.579785   \n",
       "fieldembed_token_cbow            0.455882          0.4576        0.567541   \n",
       "fieldembed_0X1_sg                0.474265        0.478752        0.577796   \n",
       "fieldembed_0X1_cbow              0.400735        0.426136        0.500063   \n",
       "fieldembed_0X1_sg-token_char     0.459559        0.483109        0.572797   \n",
       "fieldembed_0X1_cbow-token_char   0.400735         0.42643        0.500955   \n",
       "\n",
       "                                          time  \n",
       "gensim_sg                       0:09:55.008024  \n",
       "gensim_cbow                     0:05:18.418710  \n",
       "gensim_my_sg                    0:10:04.542353  \n",
       "gensim_my_cbow                  0:05:19.986750  \n",
       "nlptext_sg                      0:10:12.943225  \n",
       "nlptext_cbow                    0:02:31.671171  \n",
       "fieldembed_token_sg             0:10:18.048347  \n",
       "fieldembed_token_cbow           0:02:29.682940  \n",
       "fieldembed_0X1_sg               0:10:29.866656  \n",
       "fieldembed_0X1_cbow             0:02:35.508361  \n",
       "fieldembed_0X1_sg-token_char    0:10:33.487445  \n",
       "fieldembed_0X1_cbow-token_char  0:02:34.361514  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(D).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gensim_sg': {'sim240_spearman': 0.5102708889521944,\n",
       "  'sim297_spearman': 0.5730168515687482,\n",
       "  'ana_capital-common-countries': 0.6103896103896104,\n",
       "  'ana_city-in-state': 0.5885714285714285,\n",
       "  'ana_family': 0.48161764705882354,\n",
       "  'ana_Total accuracy': 0.5676567656765676,\n",
       "  'time': datetime.timedelta(seconds=595, microseconds=8024)},\n",
       " 'gensim_cbow': {'sim240_spearman': 0.476209763257499,\n",
       "  'sim297_spearman': 0.5813949431137461,\n",
       "  'ana_capital-common-countries': 0.5216450216450217,\n",
       "  'ana_city-in-state': 0.3657142857142857,\n",
       "  'ana_family': 0.45955882352941174,\n",
       "  'ana_Total accuracy': 0.4730473047304731,\n",
       "  'time': datetime.timedelta(seconds=318, microseconds=418710)},\n",
       " 'gensim_my_sg': {'sim240_spearman': 0.5154773369767525,\n",
       "  'sim297_spearman': 0.5834689218707985,\n",
       "  'ana_capital-common-countries': 0.6168831168831169,\n",
       "  'ana_city-in-state': 0.5942857142857143,\n",
       "  'ana_family': 0.49264705882352944,\n",
       "  'ana_Total accuracy': 0.5753575357535754,\n",
       "  'time': datetime.timedelta(seconds=604, microseconds=542353)},\n",
       " 'gensim_my_cbow': {'sim240_spearman': 0.481536196519496,\n",
       "  'sim297_spearman': 0.5849056665316643,\n",
       "  'ana_capital-common-countries': 0.5216450216450217,\n",
       "  'ana_city-in-state': 0.37142857142857144,\n",
       "  'ana_family': 0.45588235294117646,\n",
       "  'ana_Total accuracy': 0.4730473047304731,\n",
       "  'time': datetime.timedelta(seconds=319, microseconds=986750)},\n",
       " 'nlptext_sg': {'sim240_spearman': 0.49142156253734137,\n",
       "  'sim297_spearman': 0.5775639085794328,\n",
       "  'ana_capital-common-countries': 0.525974025974026,\n",
       "  'ana_city-in-state': 0.6171428571428571,\n",
       "  'ana_family': 0.4852941176470588,\n",
       "  'ana_Total accuracy': 0.5313531353135313,\n",
       "  'time': datetime.timedelta(seconds=612, microseconds=943225)},\n",
       " 'nlptext_cbow': {'sim240_spearman': 0.4485594943631713,\n",
       "  'sim297_spearman': 0.5646602824448139,\n",
       "  'ana_capital-common-countries': 0.36796536796536794,\n",
       "  'ana_city-in-state': 0.3314285714285714,\n",
       "  'ana_family': 0.4963235294117647,\n",
       "  'ana_Total accuracy': 0.39933993399339934,\n",
       "  'time': datetime.timedelta(seconds=151, microseconds=671171)},\n",
       " 'fieldembed_token_sg': {'sim240_spearman': 0.48078844041950836,\n",
       "  'sim297_spearman': 0.5797846322027205,\n",
       "  'ana_capital-common-countries': 0.48484848484848486,\n",
       "  'ana_city-in-state': 0.6171428571428571,\n",
       "  'ana_family': 0.45588235294117646,\n",
       "  'ana_Total accuracy': 0.5016501650165016,\n",
       "  'time': datetime.timedelta(seconds=618, microseconds=48347)},\n",
       " 'fieldembed_token_cbow': {'sim240_spearman': 0.4576004073175235,\n",
       "  'sim297_spearman': 0.567541303504287,\n",
       "  'ana_capital-common-countries': 0.3874458874458874,\n",
       "  'ana_city-in-state': 0.34285714285714286,\n",
       "  'ana_family': 0.45588235294117646,\n",
       "  'ana_Total accuracy': 0.39933993399339934,\n",
       "  'time': datetime.timedelta(seconds=149, microseconds=682940)},\n",
       " 'fieldembed_0X1_sg': {'sim240_spearman': 0.47875172898399704,\n",
       "  'sim297_spearman': 0.5777964160601089,\n",
       "  'ana_capital-common-countries': 0.5021645021645021,\n",
       "  'ana_city-in-state': 0.5885714285714285,\n",
       "  'ana_family': 0.4742647058823529,\n",
       "  'ana_Total accuracy': 0.5104510451045104,\n",
       "  'time': datetime.timedelta(seconds=629, microseconds=866656)},\n",
       " 'fieldembed_0X1_cbow': {'sim240_spearman': 0.42613643043321164,\n",
       "  'sim297_spearman': 0.5000632629062832,\n",
       "  'ana_capital-common-countries': 0.16233766233766234,\n",
       "  'ana_city-in-state': 0.15428571428571428,\n",
       "  'ana_family': 0.4007352941176471,\n",
       "  'ana_Total accuracy': 0.23212321232123212,\n",
       "  'time': datetime.timedelta(seconds=155, microseconds=508361)},\n",
       " 'fieldembed_0X1_sg-token_char': {'sim240_spearman': 0.4831091675436404,\n",
       "  'sim297_spearman': 0.5727970160794009,\n",
       "  'ana_capital-common-countries': 0.5151515151515151,\n",
       "  'ana_city-in-state': 0.6114285714285714,\n",
       "  'ana_family': 0.45955882352941174,\n",
       "  'ana_Total accuracy': 0.517051705170517,\n",
       "  'time': datetime.timedelta(seconds=633, microseconds=487445)},\n",
       " 'fieldembed_0X1_cbow-token_char': {'sim240_spearman': 0.426429558924676,\n",
       "  'sim297_spearman': 0.5009554655249785,\n",
       "  'ana_capital-common-countries': 0.14935064935064934,\n",
       "  'ana_city-in-state': 0.14857142857142858,\n",
       "  'ana_family': 0.4007352941176471,\n",
       "  'ana_Total accuracy': 0.22442244224422442,\n",
       "  'time': datetime.timedelta(seconds=154, microseconds=361514)}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## INIT `synneg` with Zero or Not? NOT SURE\n",
    "\n",
    "## Use `CBOW` unstandard gradient method, how to set the value `cbow_mean`? NOT SURE\n",
    "\n",
    "## Use `Dynamic` window? YES\n",
    "\n",
    "## Set `UNK` Freq to 0? YES\n",
    "\n",
    "## Use `subsampling` or `downsampling`  to handle the high freq word or not? YES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Use `Strict` dots? YES\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'File'\n",
      "corpus/wiki/wiki.txt\n",
      "Total Num of All    Tokens 31271\n",
      "The Total Number of Tokens: 31271\n",
      "Counting the number unique Tokens...          \t 2019-06-11 17:25:28.363819\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-06-11 17:25:28.368863\n",
      "\t\tThe length of DTU is: 6905 \t 2019-06-11 17:25:28.370431\n",
      "Generating the ORIGTokenIndex...       \t 2019-06-11 17:25:28.370481\n",
      "\t\tThe idx of token is: 0 \t 2019-06-11 17:25:28.370761\n",
      "\t\tDone!\n",
      "Only Keep First 447166 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 6905\n",
      "CORPUS\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 533\n",
      "SENT\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 533\n",
      "TOKEN\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 31271\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/wiki/word/Token6905/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 6905\n",
      "\t\tWrite to: data/wiki/word/Token6905/GrainUnique/token.tsv\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "\n",
    "# ########### Wiki All ###########\n",
    "CORPUSPath = 'corpus/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "MaxTextIdx = False\n",
    "MaxTokenUnique = 447166\n",
    "\n",
    "\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType,\n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel,\n",
    "                 anno, annoKW, MaxTextIdx, MaxTokenUnique)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/wiki/word/Token6905/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/wiki/word/Token6905/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/wiki/word/Token6905/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 533\n",
      "SENT\tread from pickle file : data/wiki/word/Token6905/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 533\n",
      "TOKEN\tread from pickle file : data/wiki/word/Token6905/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 31271\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/wiki/word/Token6905/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 6905\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from evals import Evaluation\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "# BOB = 'data/WikiTotal/word/Token447170/Pyramid/'\n",
    "# LGU = 'data/WikiTotal/word/Token447170/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-06-11 17:25:44.598058\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-06-11 17:25:44.598625\n",
      "\tEnd  :  2019-06-11 17:25:44.606367\n",
      "\tTotal Time: 0:00:00.007742\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-06-11 17:25:44.606620\n",
      "\tEnd  :  2019-06-11 17:25:44.622870\n",
      "\tTotal Time: 0:00:00.016250\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-06-11 17:25:44.623068\n",
      "\tEnd  :  2019-06-11 17:25:44.630540\n",
      "\tTotal Time: 0:00:00.007472\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['token']}\n",
      "[[1,\n",
      "  <fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7f184af79ba8>]]\n",
      "[[]]\n",
      "token (0, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-06-11 17:25:44.631070\n",
      "init neg with 0\n",
      "token (6905, 200)\n",
      "use_head: 1 use_sub: 0\n",
      "\tEnd  :  2019-06-11 17:25:44.714889\n",
      "\tTotal Time: 0:00:00.083819\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:00.116535\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "31271\n",
      "Start getting batch infos\n",
      "2019-06-11 17:25:44.715111\n",
      "Read info from: data/wiki/word/Token6905/Pyramid/10000_Info.p\n",
      "2019-06-11 17:25:44.716419\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.001308\n",
      "Total job number is: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Training End ......\n",
      "======== Total Time:  0:00:00.118506\n",
      "+++++End++++++ 2019-06-11 17:25:44.858013 Using: 0:00:00.259955\n",
      "0:00:00.051462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.06323958257715964,\n",
       " 'sim297_spearman': 0.03922131540100469,\n",
       " 'ana_capital-common-countries': 0.0,\n",
       " 'ana_city-in-state': 0.0,\n",
       " 'ana_family': 0.0,\n",
       " 'ana_Total accuracy': 0.0,\n",
       " 'time': datetime.timedelta(microseconds=259955)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "\n",
    "mode =  'fieldembed_0X1_neat'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.06323958257715964,\n",
       " 'sim297_spearman': 0.03922131540100469,\n",
       " 'ana_capital-common-countries': 0.0,\n",
       " 'ana_city-in-state': 0.0,\n",
       " 'ana_family': 0.0,\n",
       " 'ana_Total accuracy': 0.0,\n",
       " 'time': datetime.timedelta(microseconds=259955)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

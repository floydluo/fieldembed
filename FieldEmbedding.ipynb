{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Original Gensim Word2Vec\n",
    "\n",
    "## 1.1 SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# logging.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals import get_similarity_score\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "\n",
    "Little_Wiki_Path = 'corpus/wiki/wiki.txt'\n",
    "Total_Wiki_Path = 'corpus/WikiTotal/WikiTotal7k_v2.txt'\n",
    "\n",
    "data_input = LineSentence(Little_Wiki_Path)\n",
    "data_input = LineSentence(Total_Wiki_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-11 11:31:45.290744\n",
      "======== Build_vocab based on LineSentence....\n",
      "o--> Get Vocab Frequency from Scanning Vocab\n",
      "\tStart:  2019-05-11 11:31:45.291899\n",
      "\tEnd  :  2019-05-11 11:32:56.840986\n",
      "\tTotal Time: 0:01:11.549087\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-11 11:32:56.841337\n",
      "\tEnd  :  2019-05-11 11:32:59.159299\n",
      "\tTotal Time: 0:00:02.317962\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-11 11:32:59.159837\n",
      "\tEnd  :  2019-05-11 11:33:00.215464\n",
      "\tTotal Time: 0:00:01.055627\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-11 11:33:00.767285\n",
      "\tEnd  :  2019-05-11 11:33:01.587729\n",
      "\tTotal Time: 0:00:00.820444\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-11 11:33:01.593552\n",
      "\tEnd  :  2019-05-11 11:33:05.427904\n",
      "\tTotal Time: 0:00:03.834352\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:01:20.136198\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "======== Training End ......\n",
      "======== Total Time:  0:08:33.444202\n",
      "+++++End++++++ 2019-05-11 11:41:39.105531 Using: 0:09:53.814787\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "sg_model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, sg = 1, iter=1, workers = 8)\n",
    "e = datetime.now(); print('+++++End++++++', e, 'Using:',e - s ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = []\n",
    "# for i in data_input:\n",
    "#     L.append(i) \n",
    "# sentences = L[10]\n",
    "\n",
    "# work1, work2 = sg_model._get_thread_working_mem()\n",
    "# from fieldembed.models.word2vec_inner import train_batch_sg\n",
    "# train_batch_sg(sg_model, sentences,  alpha = 0.05, _work = work1, compute_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 / 240  word pairs appeared in the training dictionary\n",
      "sources/240.txt : SpearmanrResult(correlation=0.5010249379338645, pvalue=6.81991085481455e-16)\n",
      "279 / 297  word pairs appeared in the training dictionary\n",
      "sources/297.txt : SpearmanrResult(correlation=0.585674866818933, pvalue=4.376683806322611e-27)\n"
     ]
    }
   ],
   "source": [
    "sim_file = 'sources/240.txt'\n",
    "get_similarity_score(sim_file, token_embedding = sg_model.wv)\n",
    "\n",
    "sim_file = 'sources/297.txt'\n",
    "get_similarity_score(sim_file, token_embedding = sg_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-11 11:52:03.969928\n",
      "======== Build_vocab based on LineSentence....\n",
      "o--> Get Vocab Frequency from Scanning Vocab\n",
      "\tStart:  2019-05-11 11:52:03.970423\n",
      "\tEnd  :  2019-05-11 11:53:12.919643\n",
      "\tTotal Time: 0:01:08.949220\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-11 11:53:12.919776\n",
      "\tEnd  :  2019-05-11 11:53:14.899298\n",
      "\tTotal Time: 0:00:01.979522\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-11 11:53:14.899719\n",
      "\tEnd  :  2019-05-11 11:53:15.816528\n",
      "\tTotal Time: 0:00:00.916809\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-11 11:53:16.349709\n",
      "\tEnd  :  2019-05-11 11:53:17.154975\n",
      "\tTotal Time: 0:00:00.805266\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-11 11:53:17.160645\n",
      "\tEnd  :  2019-05-11 11:53:20.895644\n",
      "\tTotal Time: 0:00:03.734999\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:01:16.925414\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "======== Training End ......\n",
      "======== Total Time:  0:03:47.389295\n",
      "+++++End++++++ 2019-05-11 11:57:08.285583 Using: 0:05:04.315655\n"
     ]
    }
   ],
   "source": [
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "cb_model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 10, sg = 0, iter=1, workers = 8, cbow_mean=1)\n",
    "e = datetime.now(); print('+++++End++++++', e, 'Using:',e - s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 / 240  word pairs appeared in the training dictionary\n",
      "sources/240.txt : SpearmanrResult(correlation=0.4925495250226306, pvalue=2.452623470576509e-15)\n",
      "279 / 297  word pairs appeared in the training dictionary\n",
      "sources/297.txt : SpearmanrResult(correlation=0.6010947501055018, pvalue=8.528554616480385e-29)\n"
     ]
    }
   ],
   "source": [
    "sim_file = 'sources/240.txt'\n",
    "get_similarity_score(sim_file, token_embedding = cb_model.wv)\n",
    "sim_file = 'sources/297.txt'\n",
    "get_similarity_score(sim_file, token_embedding = cb_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('经济系', 0.825532078742981),\n",
       " ('医学院', 0.8067549467086792),\n",
       " ('经济学系', 0.8058873414993286),\n",
       " ('法学院', 0.8000757098197937),\n",
       " ('法律系', 0.7977119088172913),\n",
       " ('化学系', 0.7963069677352905),\n",
       " ('理工学院', 0.7959226369857788),\n",
       " ('理工大学', 0.7888524532318115),\n",
       " ('分校', 0.7874594926834106),\n",
       " ('理学院', 0.7873160243034363)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar('大学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('法学院', 0.7406864762306213),\n",
       " ('学院', 0.7253862619400024),\n",
       " ('理工学院', 0.7038729190826416),\n",
       " ('理工', 0.6829884052276611),\n",
       " ('理工大学', 0.6804722547531128),\n",
       " ('清华大学', 0.677485466003418),\n",
       " ('医学院', 0.6758483648300171),\n",
       " ('商学院', 0.6718248724937439),\n",
       " ('神学院', 0.658875048160553),\n",
       " ('师范大学', 0.6453396677970886)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model.wv.most_similar('大学')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Field Embedding With NLPText\n",
    "\n",
    "## 2.1 SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# logging.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447174/GrainUnique/token.p\n",
      "token\tthe length of it is   : 447174\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# s = datetime.now(); print(s)\n",
    "# ########### Wiki ###########\n",
    "# CORPUSPath = 'corpus/WikiTotal/'\n",
    "# # CORPUSPath = 'corpus/wiki/'\n",
    "# corpusFileIden = '.txt'\n",
    "# textType   = 'line'\n",
    "# Text2SentMethod  = 're'\n",
    "# Sent2TokenMethod = 'sep- '\n",
    "# TOKENLevel = 'word'\n",
    "# anno = False\n",
    "# annoKW = {}\n",
    "# MaxTokenUnique = 447170\n",
    "# MaxTextIdx = False\n",
    "# BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "#                  Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "#                  anno, annoKW, MaxTextIdx, MaxTokenUnique = MaxTokenUnique)\n",
    "# e = datetime.now(); print(e)\n",
    "# print(e -s )\n",
    "\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-11 12:16:00.639134\n",
      "======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-11 12:16:00.643450\n",
      "\tEnd  :  2019-05-11 12:16:01.500847\n",
      "\tTotal Time: 0:00:00.857397\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-11 12:16:01.501264\n",
      "\tEnd  :  2019-05-11 12:16:02.420595\n",
      "\tTotal Time: 0:00:00.919331\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-11 12:16:02.421027\n",
      "\tEnd  :  2019-05-11 12:16:03.093127\n",
      "\tTotal Time: 0:00:00.672100\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-11 12:16:03.093303\n",
      "\tEnd  :  2019-05-11 12:16:06.778221\n",
      "\tTotal Time: 0:00:03.684918\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.138762\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-11 12:16:06.778844\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-11 12:16:06.784792\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.005948\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:09:29.413445\n",
      "+++++End++++++ 2019-05-11 12:25:36.425951 Using: 0:09:35.786817\n"
     ]
    }
   ],
   "source": [
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "\n",
    "batch_words = 10000\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "sg_model = Word2Vec(nlptext = BasicObject, size = 200, alpha = 0.025, sg = 1, iter=1, workers = 8,batch_words = batch_words)\n",
    "e = datetime.now(); print('+++++End++++++', e, 'Using:',e - s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 / 240  word pairs appeared in the training dictionary\n",
      "sources/240.txt : SpearmanrResult(correlation=0.48415967120476044, pvalue=8.412810132777828e-15)\n",
      "279 / 297  word pairs appeared in the training dictionary\n",
      "sources/297.txt : SpearmanrResult(correlation=0.5877162822168592, pvalue=2.6300047633567268e-27)\n"
     ]
    }
   ],
   "source": [
    "# sg_model.wv.vectors\n",
    "sim_file = 'sources/240.txt'\n",
    "get_similarity_score(sim_file, token_embedding = sg_model.wv)\n",
    "\n",
    "sim_file = 'sources/297.txt'\n",
    "get_similarity_score(sim_file, token_embedding = sg_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++Start++++++ 2019-05-11 15:06:39.581727\n",
      "======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-11 15:06:39.588578\n",
      "\tEnd  :  2019-05-11 15:06:40.314871\n",
      "\tTotal Time: 0:00:00.726293\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-11 15:06:40.315332\n",
      "\tEnd  :  2019-05-11 15:06:41.246531\n",
      "\tTotal Time: 0:00:00.931199\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-11 15:06:41.246837\n",
      "\tEnd  :  2019-05-11 15:06:41.861601\n",
      "\tTotal Time: 0:00:00.614764\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-11 15:06:41.862079\n",
      "\tEnd  :  2019-05-11 15:06:45.698546\n",
      "\tTotal Time: 0:00:03.836467\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:06.117057\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "2019-05-11 15:06:45.699387\n",
      "Read info from: data/WikiTotal/word/Token447174/Pyramid/10000_Info.p\n",
      "2019-05-11 15:06:45.705375\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.005988\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:02:20.036116\n",
      "+++++End++++++ 2019-05-11 15:09:05.735490 Using: 0:02:26.153763\n"
     ]
    }
   ],
   "source": [
    "batch_words = 10000\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "cbow_model = Word2Vec(nlptext = BasicObject, size = 200, alpha = 0.025, sg = 0, iter=1, workers = 8,batch_words = batch_words)\n",
    "e = datetime.now(); print('+++++End++++++', e, 'Using:',e - s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 / 240  word pairs appeared in the training dictionary\n",
      "sources/240.txt : SpearmanrResult(correlation=0.4477823747873547, pvalue=1.2133279327426804e-12)\n",
      "279 / 297  word pairs appeared in the training dictionary\n",
      "sources/297.txt : SpearmanrResult(correlation=0.5713257961095262, pvalue=1.421612667715381e-25)\n"
     ]
    }
   ],
   "source": [
    "# sg_model.wv.vectors\n",
    "sim_file = 'sources/240.txt'\n",
    "get_similarity_score(sim_file, token_embedding = cbow_model.wv)\n",
    "\n",
    "sim_file = 'sources/297.txt'\n",
    "get_similarity_score(sim_file, token_embedding = cbow_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('经济系', 0.8247467279434204),\n",
       " ('法学院', 0.8213362693786621),\n",
       " ('化学系', 0.8110883831977844),\n",
       " ('法律系', 0.8061723113059998),\n",
       " ('理工学院', 0.7952156066894531),\n",
       " ('社会学系', 0.790824294090271),\n",
       " ('历史系', 0.7888401746749878),\n",
       " ('文学院', 0.788623571395874),\n",
       " ('理工大学', 0.7878395318984985),\n",
       " ('分校', 0.7821967601776123)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_model.wv.most_similar('大学')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('理工学院', 0.7526243925094604),\n",
       " ('法学院', 0.7520142197608948),\n",
       " ('学院', 0.7396164536476135),\n",
       " ('理工大学', 0.7357087135314941),\n",
       " ('理工', 0.7103638052940369),\n",
       " ('商学院', 0.7071322202682495),\n",
       " ('医学院', 0.6969972848892212),\n",
       " ('清华大学', 0.6865324378013611),\n",
       " ('神学院', 0.6818000078201294),\n",
       " ('师范大学', 0.6764853000640869)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.most_similar('大学')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Coding\n",
    "\n",
    "### 2.4.1 Play with NLPText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-11 15:09:41.980963\n",
      "'File'\n",
      "corpus/wiki/wiki.txt\n",
      "Total Num of All    Tokens 31271\n",
      "The Total Number of Tokens: 31271\n",
      "Counting the number unique Tokens...          \t 2019-05-11 15:09:42.044480\n",
      "\t\tDone!\n",
      "Generating Dictionary of Token Unique...\t 2019-05-11 15:09:42.049339\n",
      "\t\tThe length of DTU is: 6905 \t 2019-05-11 15:09:42.050844\n",
      "Generating the ORIGTokenIndex...       \t 2019-05-11 15:09:42.050904\n",
      "\t\tThe idx of token is: 0 \t 2019-05-11 15:09:42.051139\n",
      "\t\tDone!\n",
      "Only Keep First 447170 Tokens.\n",
      "The coverage rate is: 0.0\n",
      "Total Num of Unique Tokens 6905\n",
      "\t\tWrite to: data/wiki/word/Token6905/token.tsv\n",
      "CORPUS\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 533\n",
      "SENT\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 533\n",
      "TOKEN\tit is Dumped into file: data/wiki/word/Token6905/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 31271\n",
      "**************************************** \n",
      "\n",
      "token\tis Dumped into file: data/wiki/word/Token6905/GrainUnique/token.p\n",
      "token\tthe length of it is   : 6905\n",
      "****************************************\n",
      "2019-05-11 15:09:42.082696\n",
      "0:00:00.101733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "s = datetime.now(); print(s)\n",
    "########### Wiki ###########\n",
    "# CORPUSPath = 'corpus/WikiTotal/'\n",
    "CORPUSPath = 'corpus/wiki/'\n",
    "corpusFileIden = '.txt'\n",
    "textType   = 'line'\n",
    "Text2SentMethod  = 're'\n",
    "Sent2TokenMethod = 'sep- '\n",
    "TOKENLevel = 'word'\n",
    "anno = False\n",
    "annoKW = {}\n",
    "MaxTokenUnique = 447170\n",
    "MaxTextIdx = False\n",
    "\n",
    "BasicObject.INIT(CORPUSPath, corpusFileIden, textType, \n",
    "                 Text2SentMethod, Sent2TokenMethod, TOKENLevel, \n",
    "                 anno, annoKW, MaxTextIdx, MaxTokenUnique = MaxTokenUnique)\n",
    "e = datetime.now(); print(e)\n",
    "print(e -s )\n",
    "\n",
    "# BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "# LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "# BasicObject.INIT_FROM_PICKLE(BOB, LGU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  99,  247,  278,  312,  637,  726,  821,  902,  980, 1023],\n",
       "      dtype=uint32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentences_endidx = BasicObject.SENT['EndIDXTokens']\n",
    "sentences_endidx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  52,  402,  657,  658,   27,    6,  131,  117,   66,  402,  659,\n",
       "          4, 1401, 1088,   24, 2855,    5,   52,  402,  657,  658,   50,\n",
       "          6,    4, 2856,   73,  439,    4,  752, 2857, 1402, 1403,    6,\n",
       "       1851, 1089, 2858,  403, 1090,   30, 1852,  117,  522,  147,   13,\n",
       "        892,  659,    5, 1853, 2859,  117,  182,   34,  753, 1404,   19,\n",
       "        338,    5,  402,  659,    4, 1401, 1088,   52,  402,  657,  658,\n",
       "        372,  893, 2860,   77, 1405,    4, 1091,  304, 1854, 1092,  305,\n",
       "          4,  439, 1093,   31,  754,   16,  137, 1855,  155, 1092,  305,\n",
       "       1406,    4,   18,  125,  755, 1092,  305,   16, 2861,    4, 2862],\n",
       "      dtype=uint32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_vocidx = BasicObject.TOKEN['ORIGTokenIndex']\n",
    "# the first sentence\n",
    "tokens_vocidx[0:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</pad>', '</start>', '</end>', '</unk>', '的', '在', '年', '月', '是', '和']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LTU, DTU = BasicObject.TokenUnique\n",
    "LTU[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Generate `indexes` and `sentence_idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(tokens_vocidx)           \n",
    "total_examples  = len(sentences_endidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current batch token number: 10073\n",
      "Last sentence's end tk loc: 9993\n",
      "Current batch token number: 20007\n",
      "Last sentence's end tk loc: 19978\n",
      "Current batch token number: 30031\n",
      "Last sentence's end tk loc: 29911\n",
      "Last sentence's end tk loc: 31271\n",
      "[165, 346, 515, 533] \n",
      "\n",
      "The start and end sent loc_id: 0 165\n",
      "The token start and end loc idx in each batch: 0 9993\n",
      "9993 9993 \n",
      "\n",
      "The start and end sent loc_id: 165 346\n",
      "The token start and end loc idx in each batch: 9993 19978\n",
      "9985 9985 \n",
      "\n",
      "The start and end sent loc_id: 346 515\n",
      "The token start and end loc idx in each batch: 19978 29911\n",
      "9933 9933 \n",
      "\n",
      "The start and end sent loc_id: 515 533\n",
      "The token start and end loc idx in each batch: 29911 31271\n",
      "1360 1360 \n",
      "\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "batch_words = 10000\n",
    "\n",
    "total_words = total_words or len(tokens_vocidx)           \n",
    "total_examples  = total_examples or len(sentences_endidx)\n",
    "\n",
    "batch_end_st_idx_list = []\n",
    "job_no = 0 # job_num\n",
    "while True:\n",
    "    job_no = job_no + 1\n",
    "    batch_token_progress = job_no * batch_words  # \n",
    "\n",
    "    if batch_token_progress >= total_words:\n",
    "        \n",
    "        # if touch the bottom, go to the end and terminate the loop\n",
    "        batch_end_st_idx_list.append(total_examples)\n",
    "        # # This won't work: print('Current batch token number:', sentences_endidx[total_examples]) \n",
    "        print(\"Last sentence's end tk loc:\", sentences_endidx[total_examples-1])\n",
    "        break\n",
    "\n",
    "    # if not, find the correct end sentence loc_id for this batch\n",
    "    batch_end_st_idx = np.argmax(sentences_endidx > batch_token_progress)\n",
    "    batch_end_st_idx_list.append(batch_end_st_idx)\n",
    "    \n",
    "    print('Current batch token number:', sentences_endidx[batch_end_st_idx])\n",
    "    print(\"Last sentence's end tk loc:\", sentences_endidx[batch_end_st_idx-1])\n",
    "\n",
    "    \n",
    "print(batch_end_st_idx_list, '\\n')\n",
    "\n",
    "for idx in range(job_no):\n",
    "\n",
    "    # start and end are batch's start sentence loc_id and end sentence loc_id\n",
    "    # as python routines, batch is [start, end), left close right open\n",
    "    start = batch_end_st_idx_list[idx-1] if idx > 0 else 0\n",
    "    end   = batch_end_st_idx_list[idx]\n",
    "\n",
    "    # print(start, end)\n",
    "    # find the start sentence's start token loc_id, and\n",
    "    # find the end sentence's start token loc_id. (as the end sentence is exluded)\n",
    "    token_start = sentences_endidx[start-1] if start > 0 else 0\n",
    "    token_end   = sentences_endidx[end  -1]\n",
    "\n",
    "    indexes     = tokens_vocidx[token_start:token_end] # dtype = np.uint32\n",
    "    sentence_idx = np.array([i-token_start for i in sentences_endidx[start: end]], dtype = np.uint32)\n",
    "    print('The start and end sent loc_id:', start, end)\n",
    "    print('The token start and end loc idx in each batch:', token_start, token_end)\n",
    "    print(sentence_idx[-1], len(indexes), '\\n')\n",
    "    \n",
    "print(end == len(sentences_endidx))\n",
    "print(token_end == len(tokens_vocidx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Test Cython Function with  `indexes` and `sentence_idx`\n",
    "\n",
    "\n",
    "`indexes` and `sentences_idx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 700, 6716,  125, ...,   64,   22,  111], dtype=uint32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(indexes))\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 120,  190,  244,  266,  395,  537,  566,  686,  727,  859,  893,\n",
       "        952, 1008, 1032, 1194, 1238, 1352, 1360], dtype=uint32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentence_idx))\n",
    "sentence_idx#[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'建国 后林 一直 离群索居 从不 介入 将 帅圈子 他 不 串门 不见客 登门 来 拜访 的 人 多数 也 被 叶群挡 驾'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idx = 3\n",
    "\n",
    "start_token = sentence_idx[sent_idx-1] if sent_idx > 0 else 0\n",
    "end_token = sentence_idx[sent_idx]\n",
    "\n",
    "print(start_token, end_token)\n",
    "' '.join([LTU[i] for i in indexes[start_token:end_token]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-11 15:09:48.621704\n",
      "======== Build_vocab based on LineSentence....\n",
      "o--> Get Vocab Frequency from Scanning Vocab\n",
      "\tStart:  2019-05-11 15:09:48.622155\n",
      "\tEnd  :  2019-05-11 15:09:48.633144\n",
      "\tTotal Time: 0:00:00.010989\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-05-11 15:09:48.633348\n",
      "\tEnd  :  2019-05-11 15:09:48.642429\n",
      "\tTotal Time: 0:00:00.009081\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-05-11 15:09:48.642712\n",
      "\tEnd  :  2019-05-11 15:09:48.663886\n",
      "\tTotal Time: 0:00:00.021174\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-05-11 15:09:48.669163\n",
      "\tEnd  :  2019-05-11 15:09:48.676158\n",
      "\tTotal Time: 0:00:00.006995\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-05-11 15:09:48.676345\n",
      "\tEnd  :  2019-05-11 15:09:48.745411\n",
      "\tTotal Time: 0:00:00.069066\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:00.123636\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "======== Training End ......\n",
      "======== Total Time:  0:00:00.098370\n",
      "2019-05-11 15:09:48.921949"
     ]
    }
   ],
   "source": [
    "from fieldembed.models.word2vec_inner import train_batch_sg_nlptext\n",
    "\n",
    "from fieldembed.models.word2vec import Word2Vec, LineSentence\n",
    "from datetime import datetime\n",
    "Little_Wiki_Path = 'corpus/wiki/wiki.txt'\n",
    "data_input = LineSentence(Little_Wiki_Path)\n",
    "# data_input = LineSentence(Total_Wiki_Path)\n",
    "start = datetime.now()\n",
    "\n",
    "print(start)\n",
    "# min_count must be set as 0\n",
    "sg_model = Word2Vec(data_input, size = 200, alpha = 0.025,  min_count = 0, sg = 1, iter=1, workers = 8)\n",
    "end = datetime.now()\n",
    "time = end - start\n",
    "\n",
    "print(end)\n",
    "print(time)\n",
    "\n",
    "\n",
    "work1, work2 = sg_model._get_thread_working_mem()\n",
    "\n",
    "train_batch_sg_nlptext(sg_model, indexes, sentence_idx, alpha = 0.05, _work = work1, compute_loss = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

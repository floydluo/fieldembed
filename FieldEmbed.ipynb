{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Field Embedding Naive Version `fieldembed_token_neg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2bfbf28c331e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mLGU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/WikiTotal/word/Token447174/GrainUnique/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mBasicObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINIT_FROM_PICKLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBOB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLGU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/new_nlp/nlptext/base.py\u001b[0m in \u001b[0;36mINIT_FROM_PICKLE\u001b[0;34m(cls, Pyramid_Dir, GrainUnique_Dir)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mINIT_FROM_PICKLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyramid_Dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrainUnique_Dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;31m# TokenNum_Dir = cls.TokenNum_Dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPyramid_Dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGrainUnique_Dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m##########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "from evals import Evaluation\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "D = {} # use to save the model result\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Field Embedding Single Channel `fieldembed_token_neg_0X1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447170/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 447170\n",
      "**************************************** \n",
      "\n",
      "data/WikiTotal/word/Token447170\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447170/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447170/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus()\n",
    "print(corpus.TokenNum_Dir)\n",
    "\n",
    "#if os.path.isdir(corpus.Channel_Dir):\n",
    "#    shutil.rmtree(corpus.Channel_Dir)\n",
    "\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'pinyin':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "+++++Start++++++ 2019-06-17 23:17:17.211993\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-06-17 23:17:17.217092\n",
      "\tEnd  :  2019-06-17 23:17:18.188043\n",
      "\tTotal Time: 0:00:00.970951\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-06-17 23:17:18.188469\n",
      "\tEnd  :  2019-06-17 23:17:19.136199\n",
      "\tTotal Time: 0:00:00.947730\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-06-17 23:17:19.136405\n",
      "\tEnd  :  2019-06-17 23:17:19.830389\n",
      "\tTotal Time: 0:00:00.693984\n",
      "-------> Prepare Field Info....\n",
      "Get LookUp Table for Channel: char\n",
      "{'token': 0}\n",
      "{'token': ['char']}\n",
      "[[0, None]]\n",
      "[[[<fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7fe27cef16d8>,\n",
      "   array([   0,    1,    2, ..., 2115, 2950, 2837], dtype=uint32),\n",
      "   array([      1,       2,       3, ..., 1376236, 1376239, 1376241],\n",
      "      dtype=uint32),\n",
      "   array([1.        , 1.        , 1.        , ..., 0.25      , 0.33333334,\n",
      "       0.5       ], dtype=float32),\n",
      "   43]]]\n",
      "char (0, 200)\n",
      "use_head: 0 use_sub: 1\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-06-17 23:17:20.379219\n",
      "init neg with 0\n",
      "char (7049, 200)\n",
      "use_head: 0 use_sub: 1\n",
      "\tEnd  :  2019-06-17 23:17:20.458571\n",
      "\tTotal Time: 0:00:00.079352\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:03.246633\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "257789077\n",
      "Start getting batch infos\n",
      "2019-06-17 23:17:20.459021\n",
      "Read info from: data/WikiTotal/word/Token447170/Pyramid/10000_Info.p\n",
      "2019-06-17 23:17:20.476931\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.017910\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:08:59.198520\n",
      "+++++End++++++ 2019-06-17 23:26:19.657557 Using: 0:09:02.445564\n"
     ]
    }
   ],
   "source": [
    "selected_fields = ['char']\n",
    "mode =  'fieldembed_0X1_neat'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "from evals import Evaluation\n",
    "selected_fields_template = {k: v for k, v in CHANNEL_SETTINGS_TEMPLATE.items() if k in selected_fields}\n",
    "\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(selected_fields_template)\n",
    "# BasicObject.CHANNEL_SETTINGS\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, Field_Settings = BasicObject.CHANNEL_SETTINGS, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 2115, 2950, 2837], dtype=uint32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_wv = model.weights['char']\n",
    "char_wv.LookUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floydluo/Desktop/new_nlp/fieldembed/models/keyedvectors.py:1306: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (m / dist).astype(REAL)\n",
      "/home/floydluo/Desktop/new_nlp/fieldembed/models/keyedvectors.py:1306: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (m / dist).astype(REAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:16.763439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4418691782756045,\n",
       " 'sim297_spearman': 0.5665611035414896,\n",
       " 'ana_capital-common-countries': 0.3354978354978355,\n",
       " 'ana_city-in-state': 0.36,\n",
       " 'ana_family': 0.5661764705882353,\n",
       " 'ana_Total accuracy': 0.40924092409240925,\n",
       " 'time': datetime.timedelta(seconds=542, microseconds=445564)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wv = model.wv_neg\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "field_string = '_'.join(selected_fields)\n",
    "# D[mode+'_' + sg_or_cbow + '-' + field_string] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7957134e-03,  1.5882452e-03, -1.8345740e-03, ...,\n",
       "         7.0323318e-04, -2.8051776e-04, -4.1422943e-04],\n",
       "       [ 1.6751538e-04, -1.0281407e-03,  1.9550940e-03, ...,\n",
       "        -3.1825245e-04, -9.7429648e-04, -1.2225709e-03],\n",
       "       [ 3.6580619e-04,  1.2770864e-03,  5.9722172e-04, ...,\n",
       "        -1.8381354e-03, -1.5183592e-03, -1.1817002e-03],\n",
       "       ...,\n",
       "       [ 3.5330435e-04, -2.2741035e-04,  4.5395125e-05, ...,\n",
       "        -6.9556073e-03, -2.4317824e-03, -1.0760780e-03],\n",
       "       [-4.0874197e-03,  1.2519490e-03, -4.8138105e-04, ...,\n",
       "        -1.2221847e-02, -7.9959555e-06,  1.3628692e-03],\n",
       "       [-3.5159409e-03, -1.5763045e-04, -3.1229379e-03, ...,\n",
       "        -4.5041861e-03,  4.9837609e-04,  2.2416171e-03]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Field Embedding Single Channel `fieldembed_token_neg_M0X1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Field Embedding Single Channel `fieldembed_token_neg_M0X2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Field Embedding Single Channel `fieldembed_token_neg_M0XY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Field Embedding Single Channel `fieldembed_token_neg_M0XY_P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Field Embedding Naive Version `fieldembed_token_neg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447174/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447174/GrainUnique/token.p\n",
      "token\tthe length of it is   : 447174\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "from evals import Evaluation\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "\n",
    "D = {} # use to save the model result\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447174/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447174/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mode =  'fieldembed_token'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 0\n",
    "iter = 1\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n",
    "\n",
    "wv = model.wv\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "D[mode+'_' + sg_or_cbow] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Field Embedding Single Channel `fieldembed_token_neg_0X1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "FOLDER\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/FOLDER.p\n",
      "FOLDER\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4717592\n",
      "SENT\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 11199643\n",
      "TOKEN\tread from pickle file : data/WikiTotal/word/Token447170/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 257789077\n",
      "**************************************** \n",
      "\n",
      "token\tread from pickle file : data/WikiTotal/word/Token447170/GrainUnique/token.voc\n",
      "token\tthe length of it is   : 447170\n",
      "**************************************** \n",
      "\n",
      "data/WikiTotal/word/Token447170\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "BOB = 'data/wiki/word/Token6905/Pyramid/'\n",
    "LGU = 'data/wiki/word/Token6905/GrainUnique/'\n",
    "\n",
    "BOB = 'data/WikiTotal/word/Token447170/Pyramid/'\n",
    "LGU = 'data/WikiTotal/word/Token447170/GrainUnique/'\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(BOB, LGU)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from nlptext.corpus import Corpus\n",
    "\n",
    "\n",
    "corpus = Corpus()\n",
    "print(corpus.TokenNum_Dir)\n",
    "\n",
    "#if os.path.isdir(corpus.Channel_Dir):\n",
    "#    shutil.rmtree(corpus.Channel_Dir)\n",
    "\n",
    "\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True,'Max_Ngram': 1, 'end_grain': False},\n",
    "    'pinyin':   {'use': True,'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True,'Max_Ngram': 3, 'end_grain': True},\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deal with the Channel: char\n",
      "Current Channel is        \t char\n",
      "Current Channel Max_Ngram \t 1\n",
      "+++++Start++++++ 2019-06-11 22:32:16.946349\n",
      "!!!======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-06-11 22:32:16.957538\n",
      "\tEnd  :  2019-06-11 22:32:17.655743\n",
      "\tTotal Time: 0:00:00.698205\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-06-11 22:32:17.655852\n",
      "\tEnd  :  2019-06-11 22:32:18.588248\n",
      "\tTotal Time: 0:00:00.932396\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-06-11 22:32:18.588731\n",
      "\tEnd  :  2019-06-11 22:32:19.218365\n",
      "\tTotal Time: 0:00:00.629634\n",
      "-------> Prepare Field Info....\n",
      "{'token': 0}\n",
      "{'token': ['char']}\n",
      "[[0, None]]\n",
      "[[[<fieldembed.models.keyedvectors.Word2VecKeyedVectors object at 0x7efc1168e390>,\n",
      "   array([   0,    1,    2, ..., 2115, 2950, 2837], dtype=uint32),\n",
      "   array([      1,       2,       3, ..., 1376236, 1376239, 1376241],\n",
      "      dtype=uint32),\n",
      "   array([1.        , 1.        , 1.        , ..., 0.25      , 0.33333334,\n",
      "       0.5       ], dtype=float32),\n",
      "   43]]]\n",
      "char (0, 200)\n",
      "use_head: 0 use_sub: 1\n",
      "-------> Prepare Trainable Weight....\n",
      "o--> Prepare Trainable Parameters\n",
      "\tStart:  2019-06-11 22:32:19.356928\n",
      "init neg with 0\n",
      "char (7049, 200)\n",
      "use_head: 0 use_sub: 1\n",
      "\tEnd  :  2019-06-11 22:32:19.427411\n",
      "\tTotal Time: 0:00:00.070483\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:02.480519\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "257789077\n",
      "Start getting batch infos\n",
      "2019-06-11 22:32:19.427843\n",
      "Read info from: data/WikiTotal/word/Token447170/Pyramid/10000_Info.p\n",
      "2019-06-11 22:32:19.443899\n",
      "The time of finding batch_end_st_idx_list: 0:00:00.016056\n",
      "Total job number is: 25779\n",
      "======== Training End ......\n",
      "======== Total Time:  0:09:21.544699\n",
      "+++++End++++++ 2019-06-11 22:41:40.989321 Using: 0:09:24.042972\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20220 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c8e4e4de10ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_wv_lexical_evals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/evals.py\u001b[0m in \u001b[0;36mrun_wv_lexical_evals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_wv_lexical_evals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mpearson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_file1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sim240_spearman'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspearman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mpearson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspearman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_word_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_file2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase_insensitive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/fieldembed/models/keyedvectors.py\u001b[0m in \u001b[0;36mevaluate_word_pairs\u001b[0;34m(self, pairs, delimiter, restrict_vocab, case_insensitive, dummy4unknown)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0msimilarity_gold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Similarity from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 \u001b[0msimilarity_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Similarity from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0mspearman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_gold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/fieldembed/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m         \"\"\"\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/fieldembed/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/fieldembed/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/new_nlp/fieldembed/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetflags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20220 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "selected_fields = [ 'char']\n",
    "mode =  'fieldembed_0X1_neat'\n",
    "workers = 8\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "from fieldembed.models.fieldembed import FieldEmbedding\n",
    "from evals import Evaluation\n",
    "selected_fields_template = {k: v for k, v in CHANNEL_SETTINGS_TEMPLATE.items() if k in selected_fields}\n",
    "\n",
    "\n",
    "BasicObject.BUILD_GRAIN_UNI_AND_LOOKUP(selected_fields_template)\n",
    "# BasicObject.CHANNEL_SETTINGS\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "model = FieldEmbedding(nlptext = BasicObject, Field_Settings = BasicObject.CHANNEL_SETTINGS, size = size, alpha = alpha, mode = mode,\n",
    "                          sg = sg, iter=iter, workers = workers, batch_words = batch_words)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floydluo/Desktop/new_nlp/fieldembed/models/keyedvectors.py:1289: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return (m / dist).astype(REAL)\n",
      "/home/floydluo/Desktop/new_nlp/fieldembed/models/keyedvectors.py:1289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (m / dist).astype(REAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:17.275687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sim240_spearman': 0.4406693259012168,\n",
       " 'sim297_spearman': 0.5814490335791169,\n",
       " 'ana_capital-common-countries': 0.3354978354978355,\n",
       " 'ana_city-in-state': 0.42857142857142855,\n",
       " 'ana_family': 0.4889705882352941,\n",
       " 'ana_Total accuracy': 0.39933993399339934,\n",
       " 'time': datetime.timedelta(seconds=564, microseconds=42972)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wv = model.wv_neg\n",
    "evals = Evaluation(wv)\n",
    "\n",
    "s = datetime.now()\n",
    "d = evals.run_wv_lexical_evals()\n",
    "d['time'] = time\n",
    "e = datetime.now()\n",
    "print(e - s)\n",
    "\n",
    "\n",
    "field_string = '_'.join(selected_fields)\n",
    "# D[mode+'_' + sg_or_cbow + '-' + field_string] = d\n",
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7957134e-03,  1.5882452e-03, -1.8345740e-03, ...,\n",
       "         7.0323318e-04, -2.8051776e-04, -4.1422943e-04],\n",
       "       [ 1.6751538e-04, -1.0281407e-03,  1.9550940e-03, ...,\n",
       "        -3.1825245e-04, -9.7429648e-04, -1.2225709e-03],\n",
       "       [ 3.6580619e-04,  1.2770864e-03,  5.9722172e-04, ...,\n",
       "        -1.8381354e-03, -1.5183592e-03, -1.1817002e-03],\n",
       "       ...,\n",
       "       [ 3.5330435e-04, -2.2741035e-04,  4.5395125e-05, ...,\n",
       "        -6.9556073e-03, -2.4317824e-03, -1.0760780e-03],\n",
       "       [-4.0874197e-03,  1.2519490e-03, -4.8138105e-04, ...,\n",
       "        -1.2221847e-02, -7.9959555e-06,  1.3628692e-03],\n",
       "       [-3.5159409e-03, -1.5763045e-04, -3.1229379e-03, ...,\n",
       "        -4.5041861e-03,  4.9837609e-04,  2.2416171e-03]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Field Embedding Single Channel `fieldembed_token_neg_M0X1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Field Embedding Single Channel `fieldembed_token_neg_M0X2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Field Embedding Single Channel `fieldembed_token_neg_M0XY`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Field Embedding Single Channel `fieldembed_token_neg_M0XY_P`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

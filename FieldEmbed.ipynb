{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/WikiChinese/char/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/WikiChinese/char/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/WikiChinese/char/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 4419144\n",
      "SENT\tread from pickle file : data/WikiChinese/char/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 4419144\n",
      "TOKEN\tread from pickle file : data/WikiChinese/char/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 439594589\n",
      "**************************************** \n",
      "\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from nlptext.base import BasicObject\n",
    "from datetime import datetime\n",
    "\n",
    "from fieldembed import FieldEmbedding\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug('test')\n",
    "\n",
    "# Data_Dir = 'data/WikiEnglish/word/'\n",
    "Data_Dir = 'data/WikiChinese/char/'\n",
    "\n",
    "# this is not correct\n",
    "CHANNEL_SETTINGS_TEMPLATE = {\n",
    "    # CTX_IND\n",
    "    'token':   {'use': True, 'Max_Ngram': 1,},\n",
    "    'char':    {'use': True, 'Max_Ngram': 1, 'end_grain': False},\n",
    "    'pinyin':  {'use': True, 'Max_Ngram': 2, 'end_grain': False},\n",
    "    'subcomp': {'use': True, 'Max_Ngram': 3, 'end_grain': True},\n",
    "    'stroke':  {'use': True, 'Max_Ngram': 3, 'end_grain': True},\n",
    "    'pos':     {'use': True, 'tagScheme': 'BIOES'},\n",
    "}\n",
    "\n",
    "min_token_freq = 5\n",
    "\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir, min_token_freq)\n",
    "BasicObject.BUILD_GV_LKP(CHANNEL_SETTINGS_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "endidx = BasicObject.SENT['EndIDXTokens']\n",
    "\n",
    "L = []\n",
    "for idx, t in enumerate(endidx):\n",
    "    start = endidx[idx -1 ] if idx >0 else 0\n",
    "    end = endidx[idx]\n",
    "    leng = end - start\n",
    "    L.append(leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ..., 3855, 8810, 9256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T = np.array(L)\n",
    "T = np.sort(T)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17933"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(T[T>512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n",
      "INFO:fieldembed.model:Loading a fresh vocabulary\n",
      "INFO:fieldembed.model:effective_min_count=10 retains 143 unique words (7% of original 2000, drops 1857)\n",
      "INFO:fieldembed.model:effective_min_count=10 leaves 4876 word corpus (57% of original 8407, drops 3531)\n",
      "INFO:fieldembed.model:sample=0.001 downsamples 104 most-common words\n",
      "INFO:fieldembed.model:downsampling leaves estimated 2222 word corpus (45.6% of prior 4876)\n",
      "INFO:fieldembed.model:estimated required memory for 143 words and 200 dimensions: 300300 bytes\n",
      "INFO:fieldembed.model:resetting (initializing) layer weights\n",
      "INFO:fieldembed.model:training model with 6 workers on 143 vocabulary and 200 features, using sg=1 sample=0.001 negative=5 window=5\n",
      "INFO:fieldembed.model:\n",
      " the total_examples is:100   , the total words is:8407\n",
      "\n",
      "DEBUG:fieldembed.model:----> Worker: Job Producer loop exiting, total 1 jobs\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 0 jobs\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 5 more threads\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 0 jobs\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 0 jobs\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 0 jobs\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 0 jobs\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 4 more threads\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 3 more threads\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 2 more threads\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:fieldembed.model:o----> Worker exiting, processed 1 jobs\n",
      "INFO:fieldembed.model:Worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:fieldembed.model:EPOCH - 1 : training on 8407 raw words (2161 effective words) took 0.1s, 35435 effective words/s\n",
      "INFO:fieldembed.model:training on a 8407 raw words (2161 effective words) took 0.1s, 33301 effective words/s\n",
      "WARNING:fieldembed.model:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/wiki_cn_sample/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/wiki_cn_sample/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 1\n",
      "TEXT\tread from pickle file : data/wiki_cn_sample/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 100\n",
      "SENT\tread from pickle file : data/wiki_cn_sample/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 100\n",
      "TOKEN\tread from pickle file : data/wiki_cn_sample/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 8407\n",
      "**************************************** \n",
      "\n",
      "Deal with the Channel: token\n",
      "Current Channel is        \t token\n",
      "Current Channel Max_Ngram \t 1\n",
      "Deal with the Channel: subcomp\n",
      "Current Channel is        \t subcomp\n",
      "Current Channel Max_Ngram \t 3\n",
      "\t\tBuild Grain Uniqe and LookUp Table for channel: subcomp-n1t3e-f1\n",
      "For channel: | subcomp | build GrainUnique and LookUp\n",
      "\t\tFor Channel: subcomp \t 0 2019-07-17 09:28:19.973293\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/F10/subcomp-n1t3e-f1.voc\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/F10/subcomp-n1t3e-f1.tsv\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/F10/subcomp-n1t3e-f1.lkp\n",
      "\t\tWrite to: data/wiki_cn_sample/word/Vocab/F10/subcomp-n1t3e-f1.freq\n",
      "{'subcomp': {'Max_Ngram': 3, 'end_grain': True}, 'token': {'Max_Ngram': 1}}\n",
      "+++++Start++++++ 2019-07-17 09:28:19.985500\n",
      "6\n",
      "-------> Prepare Field Info....\n",
      "use_head: 1 use_sub: 1 use_hyper: 0\n",
      "======== Build_vocab based on NLPText....\n",
      "-------> Prepare Vocab....\n",
      "o--> Get Token Vocab Frequency from NLPText\n",
      "o--> Prepare WV's index2word, vocab...\n",
      "\tStart:  2019-07-17 09:28:19.988794\n",
      "\tEnd  :  2019-07-17 09:28:19.992140\n",
      "\tTotal Time: 0:00:00.003346\n",
      "o--> Compute Token's sampel_int...\n",
      "\tStart:  2019-07-17 09:28:19.992278\n",
      "\tEnd  :  2019-07-17 09:28:19.993128\n",
      "\tTotal Time: 0:00:00.000850\n",
      "o--> Compute Cum Table\n",
      "\tStart:  2019-07-17 09:28:19.994401\n",
      "\tEnd  :  2019-07-17 09:28:19.995517\n",
      "\tTotal Time: 0:00:00.001116\n",
      "-------> Prepare Trainable Weight....\n",
      "Init syn1neg with zeros\n",
      "token (143, 200)\n",
      "subcomp (966, 200)\n",
      "======== The Voc and Parameters are Ready!\n",
      "======== Total Time:  0:00:00.039447\n",
      "model's window size is: 5\n",
      "finish build vocab\n",
      "\n",
      "\n",
      "======== Training Start ....\n",
      "Start getting batch infos\n",
      "Total job number is: 1\n",
      "The workers number is: 6\n",
      "======== Training End ......\n",
      "======== Total Time:  0:00:00.066731\n",
      "+++++End++++++ 2019-07-17 09:28:20.094973 Using: 0:00:00.109473\n"
     ]
    }
   ],
   "source": [
    "pprint(BasicObject.CHANNEL_SETTINGS)\n",
    "mode =  'train_batch_fieldembed_negsamp'\n",
    "workers = 6\n",
    "size = 200\n",
    "batch_words = 10000\n",
    "alpha = 0.025\n",
    "sg = 1\n",
    "iter = 1\n",
    "train = True\n",
    "window = 5\n",
    "\n",
    "sg_or_cbow = 'sg' if sg else 'cbow'\n",
    "\n",
    "s = datetime.now(); print('+++++Start++++++', s)\n",
    "# end = datetime.now(); print('+++++End++++++', end, 'Using:',e - s ); \n",
    "compute_loss = True\n",
    "model = FieldEmbedding(nlptext = BasicObject, Field_Settings= BasicObject.CHANNEL_SETTINGS, \n",
    "                       size = size, train = train, alpha = alpha, mode = mode,\n",
    "                       sg = sg, iter=iter, workers = workers, batch_words = batch_words, \n",
    "                       window = window, compute_loss = compute_loss)\n",
    "e = datetime.now(); time = e - s\n",
    "print('+++++End++++++', e, 'Using:', time); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fieldembed.utils:saving FieldEmbedding object under embeddings/fieldembed/zCh_wiki_char_subcomp_model, separately None\n",
      "INFO:fieldembed.utils:not storing attribute vectors_norm\n",
      "INFO:fieldembed.utils:not storing attribute vectors_norm\n",
      "INFO:fieldembed.utils:not storing attribute cum_table\n",
      "INFO:fieldembed.utils:not storing attribute vectors_norm\n",
      "WARNING:smart_open.smart_open_lib:this function is deprecated, use smart_open.open instead\n",
      "DEBUG:smart_open.smart_open_lib:{'uri': 'embeddings/fieldembed/zCh_wiki_char_subcomp_model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'ignore_ext': False, 'transport_params': {}}\n",
      "INFO:fieldembed.utils:saved embeddings/fieldembed/zCh_wiki_char_subcomp_model\n"
     ]
    }
   ],
   "source": [
    "model.save('embeddings/fieldembed/Ch_wiki_char_subcomp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['的',\n",
       "  ',',\n",
       "  '。',\n",
       "  '、',\n",
       "  '和',\n",
       "  '数学',\n",
       "  '是',\n",
       "  '在',\n",
       "  '哲学',\n",
       "  '了',\n",
       "  '中',\n",
       "  '为',\n",
       "  '被',\n",
       "  '·',\n",
       "  '”',\n",
       "  '“',\n",
       "  '与',\n",
       "  '他',\n",
       "  '问题',\n",
       "  '认为',\n",
       "  '研究',\n",
       "  '其',\n",
       "  '对',\n",
       "  '有',\n",
       "  '(',\n",
       "  ')',\n",
       "  '上',\n",
       "  '而',\n",
       "  '科学',\n",
       "  '领域',\n",
       "  '及',\n",
       "  '理论',\n",
       "  '等',\n",
       "  ':',\n",
       "  '一',\n",
       "  '于',\n",
       "  '存在',\n",
       "  '「',\n",
       "  '」',\n",
       "  '时期',\n",
       "  '发展',\n",
       "  '也',\n",
       "  '概念',\n",
       "  '这',\n",
       "  '新',\n",
       "  '都',\n",
       "  '许多',\n",
       "  '以',\n",
       "  '并',\n",
       "  '之',\n",
       "  '且',\n",
       "  '不',\n",
       "  '世纪',\n",
       "  '\"',\n",
       "  '可以',\n",
       "  '亦',\n",
       "  '柏拉图',\n",
       "  '地',\n",
       "  '一个',\n",
       "  '数学家',\n",
       "  '即',\n",
       "  ';',\n",
       "  '从',\n",
       "  '结构',\n",
       "  '哲学家',\n",
       "  '开始',\n",
       "  '包括',\n",
       "  '他们',\n",
       "  '所',\n",
       "  '它',\n",
       "  '但',\n",
       "  '影响',\n",
       "  '到',\n",
       "  '则',\n",
       "  '产生',\n",
       "  '包含',\n",
       "  '空间',\n",
       "  '关系',\n",
       "  '以及',\n",
       "  '知识',\n",
       "  '或',\n",
       "  '来',\n",
       "  '如',\n",
       "  '年',\n",
       "  '最',\n",
       "  '称为',\n",
       "  '此',\n",
       "  '》',\n",
       "  '《',\n",
       "  '主义',\n",
       "  '这些',\n",
       "  '基础',\n",
       "  '著',\n",
       "  '-',\n",
       "  '方法',\n",
       "  '有著',\n",
       "  '其他',\n",
       "  '至',\n",
       "  '应用',\n",
       "  '证明',\n",
       "  '一些',\n",
       "  '古典',\n",
       "  '形而上学',\n",
       "  '逻辑',\n",
       "  '中国',\n",
       "  '不同',\n",
       "  '亚里士多德',\n",
       "  '由',\n",
       "  '将',\n",
       "  '一般',\n",
       "  '内',\n",
       "  '几何',\n",
       "  '自然',\n",
       "  '中世纪',\n",
       "  '精神',\n",
       "  '公理',\n",
       "  '更',\n",
       "  '近代',\n",
       "  '计算',\n",
       "  '建立',\n",
       "  '例如',\n",
       "  '主要',\n",
       "  '笛卡尔',\n",
       "  '就是',\n",
       "  '之间',\n",
       "  '形式',\n",
       "  '时代',\n",
       "  '—',\n",
       "  '思想',\n",
       "  '电脑',\n",
       "  '意思',\n",
       "  '理性',\n",
       "  '使用',\n",
       "  '学派',\n",
       "  '定理',\n",
       "  '数',\n",
       "  '统一',\n",
       "  '政治',\n",
       "  '描述',\n",
       "  '系统',\n",
       "  '普遍',\n",
       "  '变化',\n",
       "  '希腊'],\n",
       " {'的': 0,\n",
       "  ',': 1,\n",
       "  '。': 2,\n",
       "  '、': 3,\n",
       "  '和': 4,\n",
       "  '数学': 5,\n",
       "  '是': 6,\n",
       "  '在': 7,\n",
       "  '哲学': 8,\n",
       "  '了': 9,\n",
       "  '中': 10,\n",
       "  '为': 11,\n",
       "  '被': 12,\n",
       "  '·': 13,\n",
       "  '”': 14,\n",
       "  '“': 15,\n",
       "  '与': 16,\n",
       "  '他': 17,\n",
       "  '问题': 18,\n",
       "  '认为': 19,\n",
       "  '研究': 20,\n",
       "  '其': 21,\n",
       "  '对': 22,\n",
       "  '有': 23,\n",
       "  '(': 24,\n",
       "  ')': 25,\n",
       "  '上': 26,\n",
       "  '而': 27,\n",
       "  '科学': 28,\n",
       "  '领域': 29,\n",
       "  '及': 30,\n",
       "  '理论': 31,\n",
       "  '等': 32,\n",
       "  ':': 33,\n",
       "  '一': 34,\n",
       "  '于': 35,\n",
       "  '存在': 36,\n",
       "  '「': 37,\n",
       "  '」': 38,\n",
       "  '时期': 39,\n",
       "  '发展': 40,\n",
       "  '也': 41,\n",
       "  '概念': 42,\n",
       "  '这': 43,\n",
       "  '新': 44,\n",
       "  '都': 45,\n",
       "  '许多': 46,\n",
       "  '以': 47,\n",
       "  '并': 48,\n",
       "  '之': 49,\n",
       "  '且': 50,\n",
       "  '不': 51,\n",
       "  '世纪': 52,\n",
       "  '\"': 53,\n",
       "  '可以': 54,\n",
       "  '亦': 55,\n",
       "  '柏拉图': 56,\n",
       "  '地': 57,\n",
       "  '一个': 58,\n",
       "  '数学家': 59,\n",
       "  '即': 60,\n",
       "  ';': 61,\n",
       "  '从': 62,\n",
       "  '结构': 63,\n",
       "  '哲学家': 64,\n",
       "  '开始': 65,\n",
       "  '包括': 66,\n",
       "  '他们': 67,\n",
       "  '所': 68,\n",
       "  '它': 69,\n",
       "  '但': 70,\n",
       "  '影响': 71,\n",
       "  '到': 72,\n",
       "  '则': 73,\n",
       "  '产生': 74,\n",
       "  '包含': 75,\n",
       "  '空间': 76,\n",
       "  '关系': 77,\n",
       "  '以及': 78,\n",
       "  '知识': 79,\n",
       "  '或': 80,\n",
       "  '来': 81,\n",
       "  '如': 82,\n",
       "  '年': 83,\n",
       "  '最': 84,\n",
       "  '称为': 85,\n",
       "  '此': 86,\n",
       "  '》': 87,\n",
       "  '《': 88,\n",
       "  '主义': 89,\n",
       "  '这些': 90,\n",
       "  '基础': 91,\n",
       "  '著': 92,\n",
       "  '-': 93,\n",
       "  '方法': 94,\n",
       "  '有著': 95,\n",
       "  '其他': 96,\n",
       "  '至': 97,\n",
       "  '应用': 98,\n",
       "  '证明': 99,\n",
       "  '一些': 100,\n",
       "  '古典': 101,\n",
       "  '形而上学': 102,\n",
       "  '逻辑': 103,\n",
       "  '中国': 104,\n",
       "  '不同': 105,\n",
       "  '亚里士多德': 106,\n",
       "  '由': 107,\n",
       "  '将': 108,\n",
       "  '一般': 109,\n",
       "  '内': 110,\n",
       "  '几何': 111,\n",
       "  '自然': 112,\n",
       "  '中世纪': 113,\n",
       "  '精神': 114,\n",
       "  '公理': 115,\n",
       "  '更': 116,\n",
       "  '近代': 117,\n",
       "  '计算': 118,\n",
       "  '建立': 119,\n",
       "  '例如': 120,\n",
       "  '主要': 121,\n",
       "  '笛卡尔': 122,\n",
       "  '就是': 123,\n",
       "  '之间': 124,\n",
       "  '形式': 125,\n",
       "  '时代': 126,\n",
       "  '—': 127,\n",
       "  '思想': 128,\n",
       "  '电脑': 129,\n",
       "  '意思': 130,\n",
       "  '理性': 131,\n",
       "  '使用': 132,\n",
       "  '学派': 133,\n",
       "  '定理': 134,\n",
       "  '数': 135,\n",
       "  '统一': 136,\n",
       "  '政治': 137,\n",
       "  '描述': 138,\n",
       "  '系统': 139,\n",
       "  '普遍': 140,\n",
       "  '变化': 141,\n",
       "  '希腊': 142})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

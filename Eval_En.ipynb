{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Similarity and Word Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T06:37:17.109141Z",
     "start_time": "2019-08-31T06:37:15.639068Z"
    }
   },
   "outputs": [],
   "source": [
    "from fieldembed.keyedvectors import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "lexical_files = {\n",
    "    'sim': ['fieldembed/sources/eval_en/sim/ws353.txt',\n",
    "           'fieldembed/sources/eval_en/sim/ws353_relatedness.txt',\n",
    "           'fieldembed/sources/eval_en/sim/ws353_similarity.txt'],\n",
    "    'ana': ['fieldembed/sources/eval_en/ana/google_ana.txt',\n",
    "            'fieldembed/sources/eval_en/ana/msr_ana.txt',\n",
    "           ], \n",
    "}\n",
    "\n",
    "def lexical_evals(wv, lexical_files):\n",
    "    if getattr(wv, 'LKP', None) is not None:\n",
    "        raise('This is for token level embeddings')\n",
    "    d = {}\n",
    "    for task, files in lexical_files.items():\n",
    "        if task == 'sim':\n",
    "            for file in files:\n",
    "                pearson, spearman, oov_ratio = wv.evaluate_word_pairs(file, restrict_vocab=500000, case_insensitive=False)\n",
    "                d[file.split('/')[-1]] = spearman.correlation\n",
    "                \n",
    "        elif task == 'ana':\n",
    "            for file in files:\n",
    "                analogies_score, sections = wv.evaluate_word_analogies(file, restrict_vocab=500000, case_insensitive=False)\n",
    "                for section in sections:\n",
    "                    correct = len(section['correct'])\n",
    "                    total = len(section['correct']) + len(section['incorrect'])\n",
    "                    d[file.split('/')[-1] + '_' + section['section'] ] = correct/total\n",
    "    return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T06:37:18.157023Z",
     "start_time": "2019-08-31T06:37:18.141426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings/baseline/WikiEnglish/word/word2vec/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200',\n",
       " 'embeddings/baseline/WikiEnglish/word/cwe/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "base_dir = 'embeddings/baseline/WikiEnglish/'\n",
    "\n",
    "def EmbeddingModelsReader(base_dir):\n",
    "    results = [x for x in os.walk(base_dir) if x[2]]\n",
    "    d = {i[0]: i[2] for i in results}\n",
    "    L = []\n",
    "    for path, names in d.items():\n",
    "        for name in names:\n",
    "            if 'word' in name:\n",
    "                modelname = os.path.join(path, name)\n",
    "                L.append(modelname)\n",
    "                # print(modelname)\n",
    "    return L\n",
    "    \n",
    "modelnames = EmbeddingModelsReader(base_dir)\n",
    "modelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T06:37:20.960119Z",
     "start_time": "2019-08-31T06:37:20.956103Z"
    }
   },
   "outputs": [],
   "source": [
    "D_baseline = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T06:38:53.062756Z",
     "start_time": "2019-08-31T06:37:21.235390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/baseline/WikiEnglish/word/word2vec/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200\n",
      "649070 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-711a982bb0b7>\", line 10, in <module>\n",
      "    d = lexical_evals(word_vec, lexical_files)\n",
      "  File \"<ipython-input-1-c79f77b9e017>\", line 28, in lexical_evals\n",
      "    analogies_score, sections = wv.evaluate_word_analogies(file, restrict_vocab=500000, case_insensitive=False)\n",
      "  File \"/home/floydluo/Desktop/fieldembed/fieldembed/keyedvectors.py\", line 918, in evaluate_word_analogies\n",
      "    sims = self.most_similar(positive=[b, c], negative=[a], topn=5, restrict_vocab=restrict_vocab)\n",
      "  File \"/home/floydluo/Desktop/fieldembed/fieldembed/keyedvectors.py\", line 435, in most_similar\n",
      "    dists = dot(limited, mean)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/floydluo/Environments/anaconda3/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "for wv_file in modelnames:\n",
    "# wv_file = modelnames[0]\n",
    "# if True:\n",
    "    \n",
    "    # 1. load data\n",
    "    #if wv_file in D: continue \n",
    "    print(wv_file)\n",
    "    sep = ' ' if 'cwe' not in wv_file else '\\t'\n",
    "    word_vec = KeyedVectors.load_word2vec_format(wv_file, sep = sep)\n",
    "    d = lexical_evals(word_vec, lexical_files)\n",
    "    D_baseline[wv_file] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T06:38:53.063590Z",
     "start_time": "2019-08-31T06:37:32.111Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(D_baseline).T#.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T02:46:49.463783Z",
     "start_time": "2019-08-30T02:46:49.449422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings/fieldembed/WikiEnglish/word/token/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF1-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF1-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word',\n",
       " 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "base_dir = 'embeddings/fieldembed/WikiEnglish/'\n",
    "\n",
    "def EmbeddingModelsReader(base_dir):\n",
    "    results = [x for x in os.walk(base_dir) if x[2]]\n",
    "    d = {i[0]: i[2] for i in results}\n",
    "    Left, Right = [], []\n",
    "    for path, names in d.items():\n",
    "        left = []\n",
    "        for name in names:\n",
    "            if '_right_' in name and '.npy' not in name:\n",
    "                modelname = os.path.join(path, name)\n",
    "                Right.append(modelname)\n",
    "                # print(modelname)\n",
    "            elif '_left_' in name and '.npy' not in name:\n",
    "                modelname = os.path.join(path, name)\n",
    "                left.append(modelname)\n",
    "        Left.append(left)\n",
    "    return Left, Right\n",
    "    \n",
    "Left, Right = EmbeddingModelsReader(base_dir)\n",
    "Right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T06:10:52.473071Z",
     "start_time": "2019-08-29T06:10:52.470017Z"
    }
   },
   "outputs": [],
   "source": [
    "# D_all = {}\n",
    "D_pack = D_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T02:55:22.432270Z",
     "start_time": "2019-08-30T02:46:52.538841Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word\n",
      "{'ws353.txt': 0.6851494654789579, 'ws353_relatedness.txt': 0.6130753807016097, 'ws353_similarity.txt': 0.7668048154561964, 'google_ana.txt_semantics': 0.7324957167332953, 'google_ana.txt_syntax': 0.6641686182669789, 'google_ana.txt_Total accuracy': 0.6949562532166752, 'msr_ana.txt_syntactial-analogy': 0.5900383141762452, 'msr_ana.txt_Total accuracy': 0.5900383141762452}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for wv_file in Right:\n",
    "    # 1. load data\n",
    "    if wv_file in D_pack: continue \n",
    "    print(wv_file)\n",
    "    word_vec = KeyedVectors.load(wv_file)\n",
    "    d = lexical_evals(word_vec, lexical_files)\n",
    "    D_pack[wv_file] = d\n",
    "    print(d)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T02:55:22.436332Z",
     "start_time": "2019-08-30T02:55:22.434155Z"
    }
   },
   "outputs": [],
   "source": [
    "# renames = dict(zip(['embeddings/fieldembed/WikiEnglish/word/token/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF1-SmpGrT/200_right_word',\n",
    "#        'embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word',\n",
    "#        'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word',\n",
    "#        'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word'],\n",
    "#                'T TC TCPh TCPhPos'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T02:55:22.452912Z",
     "start_time": "2019-08-30T02:55:22.438100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ws353.txt</th>\n",
       "      <th>ws353_relatedness.txt</th>\n",
       "      <th>ws353_similarity.txt</th>\n",
       "      <th>google_ana.txt_Total accuracy</th>\n",
       "      <th>google_ana.txt_semantics</th>\n",
       "      <th>google_ana.txt_syntax</th>\n",
       "      <th>msr_ana.txt_Total accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF1-SmpGrT/200_right_word</th>\n",
       "      <td>0.713574</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.812294</td>\n",
       "      <td>0.769686</td>\n",
       "      <td>0.805939</td>\n",
       "      <td>0.739953</td>\n",
       "      <td>0.636899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.728100</td>\n",
       "      <td>0.652469</td>\n",
       "      <td>0.804864</td>\n",
       "      <td>0.765878</td>\n",
       "      <td>0.814620</td>\n",
       "      <td>0.725902</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.726368</td>\n",
       "      <td>0.652042</td>\n",
       "      <td>0.806802</td>\n",
       "      <td>0.746423</td>\n",
       "      <td>0.805140</td>\n",
       "      <td>0.698267</td>\n",
       "      <td>0.585028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp1e-05-nsexp0.75-th4/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.727665</td>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.811416</td>\n",
       "      <td>0.751004</td>\n",
       "      <td>0.803541</td>\n",
       "      <td>0.707916</td>\n",
       "      <td>0.598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF1-SmpGrT/200_right_word</th>\n",
       "      <td>0.681024</td>\n",
       "      <td>0.588098</td>\n",
       "      <td>0.772408</td>\n",
       "      <td>0.709727</td>\n",
       "      <td>0.718218</td>\n",
       "      <td>0.702763</td>\n",
       "      <td>0.636457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.683803</td>\n",
       "      <td>0.600199</td>\n",
       "      <td>0.769150</td>\n",
       "      <td>0.700926</td>\n",
       "      <td>0.720388</td>\n",
       "      <td>0.684965</td>\n",
       "      <td>0.621574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.682279</td>\n",
       "      <td>0.606243</td>\n",
       "      <td>0.767073</td>\n",
       "      <td>0.690221</td>\n",
       "      <td>0.710908</td>\n",
       "      <td>0.673255</td>\n",
       "      <td>0.603448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/200_right_word</th>\n",
       "      <td>0.685149</td>\n",
       "      <td>0.613075</td>\n",
       "      <td>0.766805</td>\n",
       "      <td>0.694956</td>\n",
       "      <td>0.732496</td>\n",
       "      <td>0.664169</td>\n",
       "      <td>0.590038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ws353.txt  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...   0.713574   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.728100   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.726368   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.727665   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...   0.681024   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.683803   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.682279   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...   0.685149   \n",
       "\n",
       "                                                    ws353_relatedness.txt  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...               0.625117   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.652469   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.652042   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.650995   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...               0.588098   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.600199   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.606243   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.613075   \n",
       "\n",
       "                                                    ws353_similarity.txt  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...              0.812294   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.804864   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.806802   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.811416   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...              0.772408   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.769150   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.767073   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...              0.766805   \n",
       "\n",
       "                                                    google_ana.txt_Total accuracy  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                       0.769686   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.765878   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.746423   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.751004   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                       0.709727   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.700926   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.690221   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                       0.694956   \n",
       "\n",
       "                                                    google_ana.txt_semantics  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                  0.805939   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.814620   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.805140   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.803541   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                  0.718218   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.720388   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.710908   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                  0.732496   \n",
       "\n",
       "                                                    google_ana.txt_syntax  \\\n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...               0.739953   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.725902   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.698267   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.707916   \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...               0.702763   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.684965   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.673255   \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...               0.664169   \n",
       "\n",
       "                                                    msr_ana.txt_Total accuracy  \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                    0.636899  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.611111  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.585028  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.598291  \n",
       "embeddings/fieldembed/WikiEnglish/word/token/cb...                    0.636457  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.621574  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.603448  \n",
       "embeddings/fieldembed/WikiEnglish/word/token_ch...                    0.590038  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D_fld = {}\n",
    "# for name, value in D_all.items():\n",
    "#     new_name = renames[name]\n",
    "#     D_fld[new_name] = value\n",
    "    \n",
    "import pandas as pd\n",
    "pd.DataFrame(D_pack).T[['ws353.txt', 'ws353_relatedness.txt',\n",
    "       'ws353_similarity.txt', 'google_ana.txt_Total accuracy', 'google_ana.txt_semantics',\n",
    "       'google_ana.txt_syntax', 'msr_ana.txt_Total accuracy' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T10:21:33.687099Z",
     "start_time": "2019-08-29T10:21:33.669977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ws353.txt</th>\n",
       "      <th>ws353_relatedness.txt</th>\n",
       "      <th>ws353_similarity.txt</th>\n",
       "      <th>google_ana.txt_Total accuracy</th>\n",
       "      <th>google_ana.txt_semantics</th>\n",
       "      <th>google_ana.txt_syntax</th>\n",
       "      <th>msr_ana.txt_Total accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>embeddings/baseline/WikiEnglish/word/word2vec/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200</th>\n",
       "      <td>0.651389</td>\n",
       "      <td>0.560435</td>\n",
       "      <td>0.753469</td>\n",
       "      <td>0.651158</td>\n",
       "      <td>0.650600</td>\n",
       "      <td>0.651616</td>\n",
       "      <td>0.574565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embeddings/baseline/WikiEnglish/word/cwe/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200</th>\n",
       "      <td>0.654654</td>\n",
       "      <td>0.565359</td>\n",
       "      <td>0.760095</td>\n",
       "      <td>0.648121</td>\n",
       "      <td>0.648201</td>\n",
       "      <td>0.648056</td>\n",
       "      <td>0.573534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ws353.txt  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...   0.651389   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...   0.654654   \n",
       "\n",
       "                                                    ws353_relatedness.txt  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...               0.560435   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...               0.565359   \n",
       "\n",
       "                                                    ws353_similarity.txt  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...              0.753469   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...              0.760095   \n",
       "\n",
       "                                                    google_ana.txt_Total accuracy  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...                       0.651158   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...                       0.648121   \n",
       "\n",
       "                                                    google_ana.txt_semantics  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...                  0.650600   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...                  0.648201   \n",
       "\n",
       "                                                    google_ana.txt_syntax  \\\n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...               0.651616   \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...               0.648056   \n",
       "\n",
       "                                                    msr_ana.txt_Total accuracy  \n",
       "embeddings/baseline/WikiEnglish/word/word2vec/c...                    0.574565  \n",
       "embeddings/baseline/WikiEnglish/word/cwe/cb-it5...                    0.573534  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D_baseline).T[['ws353.txt', 'ws353_relatedness.txt',\n",
    "       'ws353_similarity.txt', 'google_ana.txt_Total accuracy', 'google_ana.txt_semantics',\n",
    "       'google_ana.txt_syntax', 'msr_ana.txt_Total accuracy' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:29:54.864461Z",
     "start_time": "2019-09-04T06:29:36.724136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649068, 200)\n"
     ]
    }
   ],
   "source": [
    "from fieldembed.keyedvectors import KeyedVectors\n",
    "import os\n",
    "\n",
    "class FldEmbed_Container(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        results = [x for x in os.walk(path) if x[2]]\n",
    "        self.path = path\n",
    "        d = {i[0]: i[2] for i in results}\n",
    "        Left, Right = [], []\n",
    "        for path, names in d.items():\n",
    "            left = []\n",
    "            for name in names:\n",
    "                if '_right_' in name and '.npy' not in name:\n",
    "                    modelname = os.path.join(path, name)\n",
    "                    Right.append(modelname)\n",
    "                    # print(modelname)\n",
    "                elif '_left_' in name and '.npy' not in name:\n",
    "                    modelname = os.path.join(path, name)\n",
    "                    left.append(modelname)\n",
    "            Left.append(left)\n",
    "        self.Left = Left[0]\n",
    "        self.Right = Right[0]\n",
    "        \n",
    "        self.wv_neg = KeyedVectors.load(self.Right)\n",
    "        self.weights = {}\n",
    "        for pth in self.Left:\n",
    "            end = pth.split('/')[-1]\n",
    "            # print(end)\n",
    "            fld = 'pos_en' if 'pos_en' in end else end.split('_')[-1]\n",
    "            # print(fld)\n",
    "            if fld == 'token':\n",
    "                self.weights[fld] = self.wv_neg\n",
    "            else:\n",
    "                self.weights[fld] = KeyedVectors.load(pth)\n",
    "            \n",
    "\n",
    "            import numpy as np\n",
    "\n",
    "path = 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/'\n",
    "fldembed_container = FldEmbed_Container(path)\n",
    "print(fldembed_container.wv_neg.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:29:55.269308Z",
     "start_time": "2019-09-04T06:29:54.866156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS\tread from pickle file : data/newsgroup/word/Pyramid/CORPUS.p\n",
      "CORPUS\tthe length of it is   : 1\n",
      "GROUP\tread from pickle file : data/newsgroup/word/Pyramid/GROUP.p\n",
      "GROUP\tthe length of it is   : 20\n",
      "TEXT\tread from pickle file : data/newsgroup/word/Pyramid/TEXT.p\n",
      "TEXT\tthe length of it is   : 18756\n",
      "SENT\tread from pickle file : data/newsgroup/word/Pyramid/SENT.p\n",
      "SENT\tthe length of it is   : 18756\n",
      "TOKEN\tread from pickle file : data/newsgroup/word/Pyramid/TOKEN.p\n",
      "TOKEN\tthe length of it is   : 6857474\n",
      "**************************************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fieldembed.keyedvectors import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "from nlptext.sentence import Sentence\n",
    "from nlptext.base import BasicObject\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Data_Dir = 'data/newsgroup/word/'\n",
    "BasicObject.INIT_FROM_PICKLE(Data_Dir)\n",
    "nlptext = BasicObject\n",
    "\n",
    "\n",
    "def getsent2matrix(sent, wv):\n",
    "    token_strs = [i[0] for i in sent.get_grain_str('token')]\n",
    "    wv = wv.derivative_wv\n",
    "    wv.set_GU_and_TU()\n",
    "    TU = wv.TU # LGU in derivative wv is LTU\n",
    "    if TU is None:\n",
    "        TU = wv.GU\n",
    "    # this code is verbose\n",
    "    # TODO: how to deal with unk tokens\n",
    "    token_idxes = [TU[1].get(token_str) for token_str in token_strs if token_str in TU[1]] # 0 is not unk, to fix it in the future\n",
    "    # token_idxes = [i[0] for i in token_idxes]\n",
    "    # print(token_idxes)\n",
    "    # print(token_idxes)\n",
    "    matrix = wv.vectors[token_idxes]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def getsent2matrixfromhyper(sent, wv, fld, tagScheme = 'BIOES'):\n",
    "    token_strs = [i[0] for i in sent.get_grain_str(fld, tagScheme = tagScheme)]\n",
    "    DGU = wv.GU[1]\n",
    "    token_idxes = [DGU.get(token_str) for token_str in token_strs if token_str in DGU] # 0 is not unk, to fix it in the future\n",
    "    # token_idxes = [i[0] for i in token_idxes]\n",
    "    # print(token_idxes)\n",
    "    # print(token_idxes)\n",
    "    matrix = wv.vectors[token_idxes]\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def convert_document_to_X_and_Y(nlptext, wv, fld = 'token', tagScheme = 'BIOES'):\n",
    "    doc_num = nlptext.SENT['length']\n",
    "    \n",
    "    docmatrix = np.zeros((doc_num, wv.vector_size))\n",
    "    labels = np.zeros(doc_num, dtype = int)\n",
    "    \n",
    "    if fld not in ['pos', 'pos_en']:\n",
    "        for sentidx in range(doc_num):\n",
    "            sent = Sentence(sentidx)\n",
    "            matrix = getsent2matrix(sent, wv)\n",
    "            # matrix.append(sent)\n",
    "            docvector = np.mean(matrix, axis = 0)\n",
    "            docmatrix[sentidx] = docvector\n",
    "            labels[sentidx] = sent.IdxGroup\n",
    "    else:\n",
    "        print('hyper fields')\n",
    "        for sentidx in range(doc_num):\n",
    "            sent = Sentence(sentidx)\n",
    "            matrix = getsent2matrixfromhyper(sent, wv, fld, tagScheme)\n",
    "            # matrix.append(sent)\n",
    "            docvector = np.mean(matrix, axis = 0)\n",
    "            docmatrix[sentidx] = docvector\n",
    "            labels[sentidx] = sent.IdxGroup\n",
    "    return docmatrix, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:30:38.384562Z",
     "start_time": "2019-09-04T06:30:38.374267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/newsgroup/talk.politics.guns\n",
      "910\n",
      "corpus/newsgroup/comp.os.ms-windows.misc\n",
      "985\n",
      "corpus/newsgroup/rec.motorcycles\n",
      "994\n",
      "corpus/newsgroup/talk.politics.misc\n",
      "775\n",
      "corpus/newsgroup/comp.sys.ibm.pc.hardware\n",
      "977\n",
      "corpus/newsgroup/soc.religion.christian\n",
      "997\n",
      "corpus/newsgroup/sci.med\n",
      "987\n",
      "corpus/newsgroup/talk.religion.misc\n",
      "625\n",
      "corpus/newsgroup/comp.sys.mac.hardware\n",
      "946\n",
      "corpus/newsgroup/misc.forsale\n",
      "969\n",
      "corpus/newsgroup/alt.atheism\n",
      "792\n",
      "corpus/newsgroup/sci.crypt\n",
      "990\n",
      "corpus/newsgroup/rec.sport.hockey\n",
      "993\n",
      "corpus/newsgroup/talk.politics.mideast\n",
      "940\n",
      "corpus/newsgroup/rec.autos\n",
      "986\n",
      "corpus/newsgroup/sci.electronics\n",
      "975\n",
      "corpus/newsgroup/comp.windows.x\n",
      "976\n",
      "corpus/newsgroup/sci.space\n",
      "984\n",
      "corpus/newsgroup/rec.sport.baseball\n",
      "987\n",
      "corpus/newsgroup/comp.graphics\n",
      "968\n"
     ]
    }
   ],
   "source": [
    "from nlptext.folder import Folder\n",
    "for i in range(BasicObject.GROUP['length']):\n",
    "    f = Folder(i)\n",
    "    print(f.name)\n",
    "    s, e = f.IdxSentStartEnd\n",
    "    print(e - s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T06:30:03.697705Z",
     "start_time": "2019-09-04T06:29:55.270879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyper fields\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.1672108 , -0.01414133,  0.11257391, ...,  0.04140232,\n",
       "          0.00370974,  0.0330977 ],\n",
       "        [-0.22841461, -0.01087983,  0.11954738, ...,  0.0191622 ,\n",
       "         -0.03443069,  0.09741449],\n",
       "        [-0.1842076 , -0.00941621,  0.10770255, ...,  0.02106173,\n",
       "         -0.00480307,  0.06171349],\n",
       "        ...,\n",
       "        [-0.20721146, -0.00891671,  0.10875595, ...,  0.01490752,\n",
       "         -0.00537327,  0.05342511],\n",
       "        [-0.18790171,  0.00268668,  0.11410276, ...,  0.02615875,\n",
       "          0.0103227 ,  0.05485161],\n",
       "        [-0.20053528,  0.00200385,  0.10773578, ...,  0.02444534,\n",
       "         -0.02212308,  0.0611475 ]]), array([ 0,  0,  0, ..., 19, 19, 19]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlptext.sentence import Sentence\n",
    "\n",
    "sent = Sentence(100)\n",
    "sent\n",
    "wv = fldembed_container.weights['pos_en']\n",
    "# wv.GU[0]\n",
    "\n",
    "fld = 'pos_en'\n",
    "getsent2matrixfromhyper(sent, wv, fld, tagScheme = 'BIOES')\n",
    "\n",
    "convert_document_to_X_and_Y(nlptext, wv, fld = fld, tagScheme = 'BIOES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T13:18:44.392443Z",
     "start_time": "2019-08-30T13:10:35.109020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649070 200\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.7127835043893007\n",
      "0.003427405158593468\n",
      "{'cls_score_mean': 0.7127835043893007, 'cls_score_2std': 0.003427405158593468}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.6947639826112375\n",
      "0.016212765649845213\n",
      "{'cls_score_mean': 0.6947639826112375, 'cls_score_2std': 0.016212765649845213}\n",
      "========================================================================================== \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegressionCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-588b5a7a4cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegressionCV' is not defined"
     ]
    }
   ],
   "source": [
    "path = 'embeddings/baseline/WikiEnglish/word/word2vec/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th4/word200'\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(path)\n",
    "docmatrix, labels = convert_document_to_X_and_Y(nlptext, wv)\n",
    "X = docmatrix\n",
    "Y = labels\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30, '\\n\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word.Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T01:02:30.861123Z",
     "start_time": "2019-08-31T01:02:15.226694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649068, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "path = 'embeddings/fieldembed/WikiEnglish/word/token_char/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/'\n",
    "fldembed_container = FldEmbed_Container(path)\n",
    "print(fldembed_container.wv_neg.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T01:17:20.003201Z",
     "start_time": "2019-08-31T01:02:31.767751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.664584539997819\n",
      "0.012377715489200474\n",
      "{'cls_score_mean': 0.664584539997819, 'cls_score_2std': 0.012377715489200474}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.7298418940688691\n",
      "0.018615424533721486\n",
      "{'cls_score_mean': 0.7298418940688691, 'cls_score_2std': 0.018615424533721486}\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "0.7142349386884663\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# right word embedding\n",
    "word_embedding = fldembed_container.wv_neg\n",
    "word_vec = word_embedding\n",
    "docmatrix, labels = convert_document_to_X_and_Y(nlptext, word_vec)\n",
    "X = docmatrix\n",
    "Y = labels\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T02:01:58.817104Z",
     "start_time": "2019-08-31T01:17:20.005319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18756, 400) (18756,)\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.6418179989320805\n",
      "0.013392143451656735\n",
      "{'cls_score_mean': 0.6418179989320805, 'cls_score_2std': 0.013392143451656735}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.7535698301124573\n",
      "0.01681611458370847\n",
      "{'cls_score_mean': 0.7535698301124573, 'cls_score_2std': 0.01681611458370847}\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "0.7497778567620401\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_all = []\n",
    "for fld, wv in fldembed_container.weights.items():\n",
    "    # 3. sent clf\n",
    "    docmatrix, labels = convert_document_to_X_and_Y(nlptext, wv)\n",
    "    X_all.append(docmatrix)\n",
    "    Y = labels\n",
    "X = np.concatenate(X_all, 1)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word.Char.Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T02:02:17.930894Z",
     "start_time": "2019-08-31T02:01:58.818657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649068, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/'\n",
    "fldembed_container = FldEmbed_Container(path)\n",
    "print(fldembed_container.wv_neg.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T02:16:09.296310Z",
     "start_time": "2019-08-31T02:02:17.932733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.676952590332353\n",
      "0.010823236931206069\n",
      "{'cls_score_mean': 0.676952590332353, 'cls_score_2std': 0.010823236931206069}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.730272738014059\n",
      "0.01465113545646962\n",
      "{'cls_score_mean': 0.730272738014059, 'cls_score_2std': 0.01465113545646962}\n",
      "==========================================================================================\n",
      "==========================================================================================\n",
      "0.716012084592145\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# right word embedding\n",
    "word_embedding = fldembed_container.wv_neg\n",
    "word_vec = word_embedding\n",
    "docmatrix, labels = convert_document_to_X_and_Y(nlptext, word_vec)\n",
    "X = docmatrix\n",
    "Y = labels\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T02:46:11.680875Z",
     "start_time": "2019-08-31T02:16:09.297663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18756, 600) (18756,)\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.6375000956620048\n",
      "0.01085022399949969\n",
      "{'cls_score_mean': 0.6375000956620048, 'cls_score_2std': 0.01085022399949969}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.7682375148506465\n",
      "0.013969245847357202\n",
      "{'cls_score_mean': 0.7682375148506465, 'cls_score_2std': 0.013969245847357202}\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/home/floydluo/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b3f7b7272713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2083\u001b[0m                       \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                       )\n\u001b[0;32m-> 2085\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_encoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m             for l1_ratio in l1_ratios_)\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         max_squared_sum=max_squared_sum, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_all = []\n",
    "for fld, wv in fldembed_container.weights.items():\n",
    "    # 3. sent clf\n",
    "    docmatrix, labels = convert_document_to_X_and_Y(nlptext, wv)\n",
    "    X_all.append(docmatrix)\n",
    "    Y = labels\n",
    "X = np.concatenate(X_all, 1)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word.Char.Phoneme.Pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T03:53:17.935924Z",
     "start_time": "2019-08-31T03:52:57.497899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649068, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = 'embeddings/fieldembed/WikiEnglish/word/token_char_phoneme_pos_en/cb-it5-w5-ng10-lr0.025-smp0.001-nsexp0.75-th8/LF3-SmpGrT/'\n",
    "fldembed_container = FldEmbed_Container(path)\n",
    "print(fldembed_container.wv_neg.vectors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T03:53:17.940865Z",
     "start_time": "2019-08-31T03:53:17.937649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb77c164a8>,\n",
       " 'phoneme': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb0b25f128>,\n",
       " 'pos_en': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb11d7eeb8>,\n",
       " 'token': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb77c16828>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fldembed_container.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T02:55:10.340019Z",
     "start_time": "2019-08-31T02:47:35.535839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.6800999013760063\n",
      "0.012479174058799584\n",
      "{'cls_score_mean': 0.6800999013760063, 'cls_score_2std': 0.012479174058799584}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.7703661898916893\n",
      "0.007165291896210835\n",
      "{'cls_score_mean': 0.7703661898916893, 'cls_score_2std': 0.007165291896210835}\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e43437843743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   2083\u001b[0m                       \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m                       )\n\u001b[0;32m-> 2085\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_encoded_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m             for l1_ratio in l1_ratios_)\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         max_squared_sum=max_squared_sum, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    957\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m                 \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m                 sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# right word embedding\n",
    "word_embedding = fldembed_container.wv_neg\n",
    "word_vec = word_embedding\n",
    "docmatrix, labels = convert_document_to_X_and_Y(nlptext, word_vec)\n",
    "X = docmatrix\n",
    "Y = labels\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T03:53:17.947104Z",
     "start_time": "2019-08-31T03:53:17.942553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb77c164a8>,\n",
       " 'phoneme': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb0b25f128>,\n",
       " 'pos_en': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb11d7eeb8>,\n",
       " 'token': <fieldembed.keyedvectors.Word2VecKeyedVectors at 0x7ffb77c16828>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fldembed_container.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T04:17:50.200808Z",
     "start_time": "2019-08-31T03:53:17.949224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyper fields\n",
      "(18756, 800) (18756,)\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.6167086333710072\n",
      "0.01005100089158855\n",
      "{'cls_score_mean': 0.6167086333710072, 'cls_score_2std': 0.01005100089158855}\n",
      "========================================================================================== \n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "SVC(C=10.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovo', degree=3, gamma='auto', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=10, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n",
      "0.8012352790845343\n",
      "0.006625365255477827\n",
      "{'cls_score_mean': 0.8012352790845343, 'cls_score_2std': 0.006625365255477827}\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "X_all = []\n",
    "for fld, wv in fldembed_container.weights.items():\n",
    "    # 3. sent clf\n",
    "    docmatrix, labels = convert_document_to_X_and_Y(nlptext, wv, fld = fld, tagScheme = 'BIOES')\n",
    "    X_all.append(docmatrix)\n",
    "    Y = labels\n",
    "X = np.concatenate(X_all, 1)\n",
    "print(X.shape, Y.shape)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "clf = svm.SVC(C=10.0, \n",
    "              kernel='linear', \n",
    "              degree=3, # degree is only valid for poly\n",
    "              gamma='auto', # coefficient for 'rbf', 'poly', and 'sigmoid',\n",
    "              coef0=0.0,\n",
    "              shrinking=True,\n",
    "              probability=False,\n",
    "              tol=0.001,\n",
    "              class_weight=None,\n",
    "              cache_size=200,  \n",
    "              verbose=False,\n",
    "              max_iter=-1, \n",
    "              decision_function_shape='ovo', \n",
    "              random_state=10, )\n",
    "scores = cross_val_score(clf, X, Y, cv = 5)\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "evals_result['cls_score_mean'] = scores.mean()\n",
    "evals_result['cls_score_2std'] = scores.std() * 2\n",
    "# D[fldembed_container.path + 'WORD'] = evals_result\n",
    "\n",
    "\n",
    "print(\"===\"*30)\n",
    "print(clf)\n",
    "print(scores.mean())\n",
    "print(scores.std() * 2)\n",
    "print(evals_result)\n",
    "print(\"===\"*30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
